* 
* ==> Audit <==
* |-----------|------|----------|--------------|----------------|---------------------|---------------------|
|  Command  | Args | Profile  |     User     |    Version     |     Start Time      |      End Time       |
|-----------|------|----------|--------------|----------------|---------------------|---------------------|
| delete    |      | minikube | LEGION\pesca | v1.26.0-beta.1 | 15 Dec 22 09:05 CET | 15 Dec 22 09:05 CET |
| start     |      | minikube | LEGION\pesca | v1.26.0-beta.1 | 15 Dec 22 09:08 CET | 15 Dec 22 09:10 CET |
| stop      |      | minikube | LEGION\pesca | v1.26.0-beta.1 | 15 Dec 22 09:15 CET | 15 Dec 22 09:15 CET |
| start     |      | minikube | LEGION\pesca | v1.26.0-beta.1 | 15 Dec 22 09:15 CET | 15 Dec 22 09:15 CET |
| start     |      | minikube | LEGION\pesca | v1.26.0-beta.1 | 15 Dec 22 09:31 CET | 15 Dec 22 09:31 CET |
| start     |      | minikube | LEGION\pesca | v1.26.0-beta.1 | 15 Dec 22 09:53 CET | 15 Dec 22 09:53 CET |
| dashboard |      | minikube | LEGION\pesca | v1.26.0-beta.1 | 15 Dec 22 09:32 CET | 15 Dec 22 09:53 CET |
| logs      |      | minikube | LEGION\pesca | v1.26.0-beta.1 | 15 Dec 22 10:14 CET | 15 Dec 22 10:14 CET |
| logs      |      | minikube | LEGION\pesca | v1.26.0-beta.1 | 15 Dec 22 10:14 CET | 15 Dec 22 10:14 CET |
| logs      |      | minikube | LEGION\pesca | v1.26.0-beta.1 | 15 Dec 22 11:13 CET | 15 Dec 22 11:13 CET |
|-----------|------|----------|--------------|----------------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2022/12/15 09:53:13
Running on machine: LEGION
Binary: Built with gc go1.18.2 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1215 09:53:13.881504   10680 out.go:296] Setting OutFile to fd 1040 ...
I1215 09:53:13.881504   10680 out.go:343] TERM=xterm,COLORTERM=, which probably does not support color
I1215 09:53:13.881504   10680 out.go:309] Setting ErrFile to fd 1040...
I1215 09:53:13.881504   10680 out.go:343] TERM=xterm,COLORTERM=, which probably does not support color
I1215 09:53:13.892083   10680 out.go:303] Setting JSON to false
I1215 09:53:13.896086   10680 start.go:115] hostinfo: {"hostname":"LEGION","uptime":2724,"bootTime":1671091669,"procs":252,"os":"windows","platform":"Microsoft Windows 11 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.22621 Build 22621","kernelVersion":"10.0.22621 Build 22621","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"452a41d9-3cb9-4c8b-9eeb-83c2dbc912d8"}
W1215 09:53:13.896086   10680 start.go:123] gopshost.Virtualization returned error: not implemented yet
I1215 09:53:13.897639   10680 out.go:177] * minikube v1.26.0-beta.1 on Microsoft Windows 11 Pro 10.0.22621 Build 22621
I1215 09:53:13.898686   10680 notify.go:193] Checking for updates...
I1215 09:53:13.899211   10680 config.go:178] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.23.6
I1215 09:53:13.899211   10680 driver.go:358] Setting default libvirt URI to qemu:///system
I1215 09:53:14.014157   10680 docker.go:137] docker version: linux-20.10.20
I1215 09:53:14.016911   10680 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1215 09:53:14.414064   10680 info.go:265] docker info: {ID:AQPR:X6MT:NNZC:J3IJ:5YGU:ICFB:FRFY:NXFW:QDOH:6SYP:OF2S:RRVH Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:60 OomKillDisable:true NGoroutines:56 SystemTime:2022-12-15 08:53:14.093459925 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:5 KernelVersion:5.10.16.3-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:25137364992 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.20 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6 Expected:9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.9.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.12.1] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.0.3] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.13] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.21.0]] Warnings:<nil>}}
I1215 09:53:14.415696   10680 out.go:177] * Using the docker driver based on existing profile
I1215 09:53:14.416738   10680 start.go:284] selected driver: docker
I1215 09:53:14.416738   10680 start.go:806] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.31@sha256:c3375f1b260bd936aa532a0c749626e07d94ab129a7f2395e95345aa04ca708c Memory:12200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.23.6 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.23.6 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\pesca:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false}
I1215 09:53:14.416738   10680 start.go:817] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1215 09:53:14.426122   10680 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1215 09:53:14.798327   10680 info.go:265] docker info: {ID:AQPR:X6MT:NNZC:J3IJ:5YGU:ICFB:FRFY:NXFW:QDOH:6SYP:OF2S:RRVH Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:60 OomKillDisable:true NGoroutines:56 SystemTime:2022-12-15 08:53:14.504977655 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:5 KernelVersion:5.10.16.3-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:25137364992 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.20 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6 Expected:9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.9.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.12.1] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.0.3] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.13] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.21.0]] Warnings:<nil>}}
I1215 09:53:14.859106   10680 cni.go:95] Creating CNI manager for ""
I1215 09:53:14.859106   10680 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I1215 09:53:14.859106   10680 start_flags.go:306] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.31@sha256:c3375f1b260bd936aa532a0c749626e07d94ab129a7f2395e95345aa04ca708c Memory:12200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.23.6 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.23.6 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\pesca:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false}
I1215 09:53:14.860657   10680 out.go:177] * Starting control plane node minikube in cluster minikube
I1215 09:53:14.861734   10680 cache.go:120] Beginning downloading kic base image for docker with docker
I1215 09:53:14.862256   10680 out.go:177] * Pulling base image ...
I1215 09:53:14.863313   10680 image.go:75] Checking for gcr.io/k8s-minikube/kicbase:v0.0.31@sha256:c3375f1b260bd936aa532a0c749626e07d94ab129a7f2395e95345aa04ca708c in local docker daemon
I1215 09:53:14.863313   10680 preload.go:132] Checking if preload exists for k8s version v1.23.6 and runtime docker
I1215 09:53:14.863313   10680 preload.go:148] Found local preload: C:\Users\pesca\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.23.6-docker-overlay2-amd64.tar.lz4
I1215 09:53:14.863313   10680 cache.go:57] Caching tarball of preloaded images
I1215 09:53:14.863313   10680 preload.go:174] Found C:\Users\pesca\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.23.6-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1215 09:53:14.863313   10680 cache.go:60] Finished verifying existence of preloaded tar for  v1.23.6 on docker
I1215 09:53:14.863845   10680 profile.go:148] Saving config to C:\Users\pesca\.minikube\profiles\minikube\config.json ...
I1215 09:53:14.984197   10680 image.go:79] Found gcr.io/k8s-minikube/kicbase:v0.0.31@sha256:c3375f1b260bd936aa532a0c749626e07d94ab129a7f2395e95345aa04ca708c in local docker daemon, skipping pull
I1215 09:53:14.984197   10680 cache.go:141] gcr.io/k8s-minikube/kicbase:v0.0.31@sha256:c3375f1b260bd936aa532a0c749626e07d94ab129a7f2395e95345aa04ca708c exists in daemon, skipping load
I1215 09:53:14.984197   10680 cache.go:206] Successfully downloaded all kic artifacts
I1215 09:53:14.984197   10680 start.go:352] acquiring machines lock for minikube: {Name:mk1da495867aa1987ed6247ea5adf9ba4bf9b2af Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1215 09:53:14.984197   10680 start.go:356] acquired machines lock for "minikube" in 0s
I1215 09:53:14.984197   10680 start.go:94] Skipping create...Using existing machine configuration
I1215 09:53:14.984197   10680 fix.go:55] fixHost starting: 
I1215 09:53:14.990497   10680 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1215 09:53:15.106199   10680 fix.go:103] recreateIfNeeded on minikube: state=Running err=<nil>
W1215 09:53:15.106268   10680 fix.go:129] unexpected machine state, will restart: <nil>
I1215 09:53:15.107842   10680 out.go:177] * Updating the running docker "minikube" container ...
I1215 09:53:15.108369   10680 machine.go:88] provisioning docker machine ...
I1215 09:53:15.108369   10680 ubuntu.go:169] provisioning hostname "minikube"
I1215 09:53:15.110989   10680 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1215 09:53:15.229704   10680 main.go:134] libmachine: Using SSH client type: native
I1215 09:53:15.230223   10680 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x8e2ea0] 0x8e5d00 <nil>  [] 0s} 127.0.0.1 50767 <nil> <nil>}
I1215 09:53:15.230223   10680 main.go:134] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1215 09:53:15.365968   10680 main.go:134] libmachine: SSH cmd err, output: <nil>: minikube

I1215 09:53:15.368654   10680 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1215 09:53:15.475479   10680 main.go:134] libmachine: Using SSH client type: native
I1215 09:53:15.475989   10680 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x8e2ea0] 0x8e5d00 <nil>  [] 0s} 127.0.0.1 50767 <nil> <nil>}
I1215 09:53:15.475989   10680 main.go:134] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1215 09:53:15.606900   10680 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I1215 09:53:15.606900   10680 ubuntu.go:175] set auth options {CertDir:C:\Users\pesca\.minikube CaCertPath:C:\Users\pesca\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\pesca\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\pesca\.minikube\machines\server.pem ServerKeyPath:C:\Users\pesca\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\pesca\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\pesca\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\pesca\.minikube}
I1215 09:53:15.606900   10680 ubuntu.go:177] setting up certificates
I1215 09:53:15.606900   10680 provision.go:83] configureAuth start
I1215 09:53:15.609519   10680 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1215 09:53:15.720054   10680 provision.go:138] copyHostCerts
I1215 09:53:15.720054   10680 exec_runner.go:144] found C:\Users\pesca\.minikube/ca.pem, removing ...
I1215 09:53:15.720054   10680 exec_runner.go:207] rm: C:\Users\pesca\.minikube\ca.pem
I1215 09:53:15.720560   10680 exec_runner.go:151] cp: C:\Users\pesca\.minikube\certs\ca.pem --> C:\Users\pesca\.minikube/ca.pem (1074 bytes)
I1215 09:53:15.721138   10680 exec_runner.go:144] found C:\Users\pesca\.minikube/cert.pem, removing ...
I1215 09:53:15.721138   10680 exec_runner.go:207] rm: C:\Users\pesca\.minikube\cert.pem
I1215 09:53:15.721138   10680 exec_runner.go:151] cp: C:\Users\pesca\.minikube\certs\cert.pem --> C:\Users\pesca\.minikube/cert.pem (1119 bytes)
I1215 09:53:15.721642   10680 exec_runner.go:144] found C:\Users\pesca\.minikube/key.pem, removing ...
I1215 09:53:15.721642   10680 exec_runner.go:207] rm: C:\Users\pesca\.minikube\key.pem
I1215 09:53:15.721642   10680 exec_runner.go:151] cp: C:\Users\pesca\.minikube\certs\key.pem --> C:\Users\pesca\.minikube/key.pem (1679 bytes)
I1215 09:53:15.722161   10680 provision.go:112] generating server cert: C:\Users\pesca\.minikube\machines\server.pem ca-key=C:\Users\pesca\.minikube\certs\ca.pem private-key=C:\Users\pesca\.minikube\certs\ca-key.pem org=pesca.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1215 09:53:15.786061   10680 provision.go:172] copyRemoteCerts
I1215 09:53:15.790274   10680 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1215 09:53:15.793344   10680 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1215 09:53:15.906217   10680 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50767 SSHKeyPath:C:\Users\pesca\.minikube\machines\minikube\id_rsa Username:docker}
I1215 09:53:16.010019   10680 ssh_runner.go:362] scp C:\Users\pesca\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I1215 09:53:16.023652   10680 ssh_runner.go:362] scp C:\Users\pesca\.minikube\machines\server.pem --> /etc/docker/server.pem (1200 bytes)
I1215 09:53:16.037270   10680 ssh_runner.go:362] scp C:\Users\pesca\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1215 09:53:16.049860   10680 provision.go:86] duration metric: configureAuth took 442.96ms
I1215 09:53:16.049860   10680 ubuntu.go:193] setting minikube options for container-runtime
I1215 09:53:16.050389   10680 config.go:178] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.23.6
I1215 09:53:16.052474   10680 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1215 09:53:16.168884   10680 main.go:134] libmachine: Using SSH client type: native
I1215 09:53:16.169461   10680 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x8e2ea0] 0x8e5d00 <nil>  [] 0s} 127.0.0.1 50767 <nil> <nil>}
I1215 09:53:16.169461   10680 main.go:134] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1215 09:53:16.296890   10680 main.go:134] libmachine: SSH cmd err, output: <nil>: overlay

I1215 09:53:16.296890   10680 ubuntu.go:71] root file system type: overlay
I1215 09:53:16.296890   10680 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1215 09:53:16.299511   10680 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1215 09:53:16.417666   10680 main.go:134] libmachine: Using SSH client type: native
I1215 09:53:16.418188   10680 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x8e2ea0] 0x8e5d00 <nil>  [] 0s} 127.0.0.1 50767 <nil> <nil>}
I1215 09:53:16.418188   10680 main.go:134] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1215 09:53:16.506883   10680 main.go:134] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1215 09:53:16.509567   10680 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1215 09:53:16.615940   10680 main.go:134] libmachine: Using SSH client type: native
I1215 09:53:16.615940   10680 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x8e2ea0] 0x8e5d00 <nil>  [] 0s} 127.0.0.1 50767 <nil> <nil>}
I1215 09:53:16.616461   10680 main.go:134] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1215 09:53:16.700098   10680 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I1215 09:53:16.700098   10680 machine.go:91] provisioned docker machine in 1.5917297s
I1215 09:53:16.700098   10680 start.go:306] post-start starting for "minikube" (driver="docker")
I1215 09:53:16.700098   10680 start.go:316] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1215 09:53:16.704304   10680 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1215 09:53:16.706902   10680 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1215 09:53:16.829616   10680 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50767 SSHKeyPath:C:\Users\pesca\.minikube\machines\minikube\id_rsa Username:docker}
I1215 09:53:16.931181   10680 ssh_runner.go:195] Run: cat /etc/os-release
I1215 09:53:16.933824   10680 main.go:134] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1215 09:53:16.933824   10680 main.go:134] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1215 09:53:16.933824   10680 main.go:134] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1215 09:53:16.933824   10680 info.go:137] Remote host: Ubuntu 20.04.4 LTS
I1215 09:53:16.933824   10680 filesync.go:126] Scanning C:\Users\pesca\.minikube\addons for local assets ...
I1215 09:53:16.934348   10680 filesync.go:126] Scanning C:\Users\pesca\.minikube\files for local assets ...
I1215 09:53:16.934348   10680 start.go:309] post-start completed in 234.2498ms
I1215 09:53:16.934875   10680 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1215 09:53:16.937539   10680 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1215 09:53:17.046107   10680 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50767 SSHKeyPath:C:\Users\pesca\.minikube\machines\minikube\id_rsa Username:docker}
I1215 09:53:17.139640   10680 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1215 09:53:17.142781   10680 fix.go:57] fixHost completed within 2.1585838s
I1215 09:53:17.142781   10680 start.go:81] releasing machines lock for "minikube", held for 2.1585838s
I1215 09:53:17.145934   10680 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1215 09:53:17.247683   10680 ssh_runner.go:195] Run: curl -sS -m 2 https://k8s.gcr.io/
I1215 09:53:17.250347   10680 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1215 09:53:17.251409   10680 ssh_runner.go:195] Run: systemctl --version
I1215 09:53:17.254050   10680 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1215 09:53:17.370704   10680 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50767 SSHKeyPath:C:\Users\pesca\.minikube\machines\minikube\id_rsa Username:docker}
I1215 09:53:17.381261   10680 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50767 SSHKeyPath:C:\Users\pesca\.minikube\machines\minikube\id_rsa Username:docker}
I1215 09:53:17.703434   10680 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1215 09:53:17.719233   10680 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1215 09:53:17.726608   10680 cruntime.go:273] skipping containerd shutdown because we are bound to it
I1215 09:53:17.730236   10680 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1215 09:53:17.738127   10680 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/dockershim.sock
image-endpoint: unix:///var/run/dockershim.sock
" | sudo tee /etc/crictl.yaml"
I1215 09:53:17.752347   10680 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1215 09:53:17.843738   10680 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1215 09:53:17.945680   10680 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1215 09:53:17.956723   10680 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1215 09:53:18.067512   10680 ssh_runner.go:195] Run: sudo systemctl start docker
I1215 09:53:18.078490   10680 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1215 09:53:18.107146   10680 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1215 09:53:18.136044   10680 out.go:204] * Preparing Kubernetes v1.23.6 on Docker 20.10.15 ...
I1215 09:53:18.138155   10680 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I1215 09:53:18.341255   10680 network.go:96] got host ip for mount in container by digging dns: 192.168.65.2
I1215 09:53:18.341794   10680 ssh_runner.go:195] Run: grep 192.168.65.2	host.minikube.internal$ /etc/hosts
I1215 09:53:18.348416   10680 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1215 09:53:18.463870   10680 preload.go:132] Checking if preload exists for k8s version v1.23.6 and runtime docker
I1215 09:53:18.466449   10680 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1215 09:53:18.491423   10680 docker.go:610] Got preloaded images: -- stdout --
ghcr.io/eddiehubcommunity/linkfree:latest
k8s.gcr.io/kube-apiserver:v1.23.6
k8s.gcr.io/kube-controller-manager:v1.23.6
k8s.gcr.io/kube-proxy:v1.23.6
k8s.gcr.io/kube-scheduler:v1.23.6
kubernetesui/dashboard:<none>
k8s.gcr.io/etcd:3.5.1-0
k8s.gcr.io/coredns/coredns:v1.8.6
k8s.gcr.io/pause:3.6
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1215 09:53:18.491423   10680 docker.go:541] Images already preloaded, skipping extraction
I1215 09:53:18.494218   10680 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1215 09:53:18.516558   10680 docker.go:610] Got preloaded images: -- stdout --
ghcr.io/eddiehubcommunity/linkfree:latest
k8s.gcr.io/kube-apiserver:v1.23.6
k8s.gcr.io/kube-scheduler:v1.23.6
k8s.gcr.io/kube-proxy:v1.23.6
k8s.gcr.io/kube-controller-manager:v1.23.6
kubernetesui/dashboard:<none>
k8s.gcr.io/etcd:3.5.1-0
k8s.gcr.io/coredns/coredns:v1.8.6
k8s.gcr.io/pause:3.6
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1215 09:53:18.516558   10680 cache_images.go:84] Images are preloaded, skipping loading
I1215 09:53:18.519180   10680 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1215 09:53:18.571616   10680 cni.go:95] Creating CNI manager for ""
I1215 09:53:18.571616   10680 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I1215 09:53:18.571616   10680 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1215 09:53:18.571616   10680 kubeadm.go:158] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.23.6 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/dockershim.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NoTaintMaster:true NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[]}
I1215 09:53:18.571616   10680 kubeadm.go:162] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.23.6
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1215 09:53:18.571616   10680 kubeadm.go:961] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.23.6/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=docker --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.23.6 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1215 09:53:18.575301   10680 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.23.6
I1215 09:53:18.582203   10680 binaries.go:44] Found k8s binaries, skipping transfer
I1215 09:53:18.587031   10680 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1215 09:53:18.592704   10680 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (334 bytes)
I1215 09:53:18.602717   10680 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1215 09:53:18.612703   10680 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2030 bytes)
I1215 09:53:18.623177   10680 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1215 09:53:18.625780   10680 certs.go:54] Setting up C:\Users\pesca\.minikube\profiles\minikube for IP: 192.168.49.2
I1215 09:53:18.625780   10680 certs.go:182] skipping minikubeCA CA generation: C:\Users\pesca\.minikube\ca.key
I1215 09:53:18.625780   10680 certs.go:182] skipping proxyClientCA CA generation: C:\Users\pesca\.minikube\proxy-client-ca.key
I1215 09:53:18.626305   10680 certs.go:298] skipping minikube-user signed cert generation: C:\Users\pesca\.minikube\profiles\minikube\client.key
I1215 09:53:18.626305   10680 certs.go:298] skipping minikube signed cert generation: C:\Users\pesca\.minikube\profiles\minikube\apiserver.key.dd3b5fb2
I1215 09:53:18.626305   10680 certs.go:298] skipping aggregator signed cert generation: C:\Users\pesca\.minikube\profiles\minikube\proxy-client.key
I1215 09:53:18.626828   10680 certs.go:388] found cert: C:\Users\pesca\.minikube\certs\C:\Users\pesca\.minikube\certs\ca-key.pem (1679 bytes)
I1215 09:53:18.626828   10680 certs.go:388] found cert: C:\Users\pesca\.minikube\certs\C:\Users\pesca\.minikube\certs\ca.pem (1074 bytes)
I1215 09:53:18.626828   10680 certs.go:388] found cert: C:\Users\pesca\.minikube\certs\C:\Users\pesca\.minikube\certs\cert.pem (1119 bytes)
I1215 09:53:18.626828   10680 certs.go:388] found cert: C:\Users\pesca\.minikube\certs\C:\Users\pesca\.minikube\certs\key.pem (1679 bytes)
I1215 09:53:18.628404   10680 ssh_runner.go:362] scp C:\Users\pesca\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1215 09:53:18.643057   10680 ssh_runner.go:362] scp C:\Users\pesca\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1215 09:53:18.656122   10680 ssh_runner.go:362] scp C:\Users\pesca\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1215 09:53:18.670309   10680 ssh_runner.go:362] scp C:\Users\pesca\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1215 09:53:18.683913   10680 ssh_runner.go:362] scp C:\Users\pesca\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1215 09:53:18.698050   10680 ssh_runner.go:362] scp C:\Users\pesca\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1215 09:53:18.711692   10680 ssh_runner.go:362] scp C:\Users\pesca\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1215 09:53:18.726315   10680 ssh_runner.go:362] scp C:\Users\pesca\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1215 09:53:18.739451   10680 ssh_runner.go:362] scp C:\Users\pesca\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1215 09:53:18.753513   10680 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (752 bytes)
I1215 09:53:18.764005   10680 ssh_runner.go:195] Run: openssl version
I1215 09:53:18.772504   10680 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1215 09:53:18.779807   10680 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1215 09:53:18.782933   10680 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Dec 15 08:10 /usr/share/ca-certificates/minikubeCA.pem
I1215 09:53:18.783446   10680 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1215 09:53:18.790861   10680 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1215 09:53:18.797598   10680 kubeadm.go:395] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.31@sha256:c3375f1b260bd936aa532a0c749626e07d94ab129a7f2395e95345aa04ca708c Memory:12200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.23.6 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.23.6 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\pesca:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false}
I1215 09:53:18.800245   10680 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1215 09:53:18.827156   10680 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1215 09:53:18.832950   10680 kubeadm.go:410] found existing configuration files, will attempt cluster restart
I1215 09:53:18.832950   10680 kubeadm.go:626] restartCluster start
I1215 09:53:18.836607   10680 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1215 09:53:18.842358   10680 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1215 09:53:18.845502   10680 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1215 09:53:18.954818   10680 kubeconfig.go:92] found "minikube" server: "https://127.0.0.1:50766"
I1215 09:53:18.959506   10680 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1215 09:53:18.965608   10680 api_server.go:165] Checking apiserver status ...
I1215 09:53:18.968731   10680 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1215 09:53:18.980271   10680 ssh_runner.go:195] Run: sudo egrep ^[0-9]+:freezer: /proc/1760/cgroup
I1215 09:53:18.987632   10680 api_server.go:181] apiserver freezer: "7:freezer:/docker/9c01f6e92eefd444eac911c5c02d8a939fcef2467ef2d79036301238d9cfef46/kubepods/burstable/pod0291134be0a028f0269222c568f43f10/7319aeb72e88fbdfe27a17ffd3af81651cb591abc9f97acde41dc58e4368df61"
I1215 09:53:18.991270   10680 ssh_runner.go:195] Run: sudo cat /sys/fs/cgroup/freezer/docker/9c01f6e92eefd444eac911c5c02d8a939fcef2467ef2d79036301238d9cfef46/kubepods/burstable/pod0291134be0a028f0269222c568f43f10/7319aeb72e88fbdfe27a17ffd3af81651cb591abc9f97acde41dc58e4368df61/freezer.state
I1215 09:53:18.997014   10680 api_server.go:203] freezer state: "THAWED"
I1215 09:53:18.997014   10680 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:50766/healthz ...
I1215 09:53:19.001787   10680 api_server.go:266] https://127.0.0.1:50766/healthz returned 200:
ok
I1215 09:53:19.011684   10680 system_pods.go:86] 7 kube-system pods found
I1215 09:53:19.011684   10680 system_pods.go:89] "coredns-64897985d-2t96b" [32e81d98-4c0e-46ec-9329-ebda5ba6b5cc] Running
I1215 09:53:19.011684   10680 system_pods.go:89] "etcd-minikube" [a7cd0a9c-2945-4281-89c2-e71a8e0e9fd3] Running
I1215 09:53:19.011684   10680 system_pods.go:89] "kube-apiserver-minikube" [96528e10-3258-4c28-b5e5-8794dea3e3c2] Running
I1215 09:53:19.011684   10680 system_pods.go:89] "kube-controller-manager-minikube" [78dcd395-4580-4a7f-9063-0254ec6a43ce] Running
I1215 09:53:19.011684   10680 system_pods.go:89] "kube-proxy-fgd5f" [a4c5a07f-374d-452e-b646-47f5a8ceb761] Running
I1215 09:53:19.011684   10680 system_pods.go:89] "kube-scheduler-minikube" [58ad3b0c-db5b-423c-b2a3-39f2e3e46da4] Running
I1215 09:53:19.011684   10680 system_pods.go:89] "storage-provisioner" [959b81f3-1586-4c43-8f0e-83e2f39864dd] Running
I1215 09:53:19.013260   10680 api_server.go:140] control plane version: v1.23.6
I1215 09:53:19.013260   10680 kubeadm.go:620] The running cluster does not require reconfiguration: 127.0.0.1
I1215 09:53:19.013260   10680 kubeadm.go:674] Taking a shortcut, as the cluster seems to be properly configured
I1215 09:53:19.013260   10680 kubeadm.go:630] restartCluster took 180.31ms
I1215 09:53:19.013260   10680 kubeadm.go:397] StartCluster complete in 215.6625ms
I1215 09:53:19.013260   10680 settings.go:142] acquiring lock: {Name:mk61387252910ce4c5d6e5f30284d1b7c928f845 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1215 09:53:19.013260   10680 settings.go:150] Updating kubeconfig:  C:\Users\pesca\.kube\config
I1215 09:53:19.013784   10680 lock.go:35] WriteFile acquiring C:\Users\pesca\.kube\config: {Name:mk2bd3c16d538c0ca2dddcba24413fb54a5704f6 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1215 09:53:19.022155   10680 kapi.go:244] deployment "coredns" in namespace "kube-system" and context "minikube" rescaled to 1
I1215 09:53:19.022155   10680 start.go:208] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.23.6 ContainerRuntime:docker ControlPlane:true Worker:true}
I1215 09:53:19.022155   10680 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.6/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1215 09:53:19.023208   10680 out.go:177] * Verifying Kubernetes components...
I1215 09:53:19.022155   10680 addons.go:415] enableAddons start: toEnable=map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false], additional=[]
I1215 09:53:19.022688   10680 config.go:178] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.23.6
I1215 09:53:19.023208   10680 addons.go:65] Setting storage-provisioner=true in profile "minikube"
I1215 09:53:19.023208   10680 addons.go:65] Setting default-storageclass=true in profile "minikube"
I1215 09:53:19.023208   10680 addons.go:65] Setting dashboard=true in profile "minikube"
I1215 09:53:19.024249   10680 addons.go:153] Setting addon storage-provisioner=true in "minikube"
W1215 09:53:19.024249   10680 addons.go:165] addon storage-provisioner should already be in state true
I1215 09:53:19.024249   10680 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1215 09:53:19.024249   10680 addons.go:153] Setting addon dashboard=true in "minikube"
W1215 09:53:19.024249   10680 addons.go:165] addon dashboard should already be in state true
I1215 09:53:19.024249   10680 host.go:66] Checking if "minikube" exists ...
I1215 09:53:19.024249   10680 host.go:66] Checking if "minikube" exists ...
I1215 09:53:19.028415   10680 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1215 09:53:19.030506   10680 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1215 09:53:19.030570   10680 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1215 09:53:19.030570   10680 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1215 09:53:19.158945   10680 addons.go:153] Setting addon default-storageclass=true in "minikube"
W1215 09:53:19.158945   10680 addons.go:165] addon default-storageclass should already be in state true
I1215 09:53:19.159470   10680 host.go:66] Checking if "minikube" exists ...
I1215 09:53:19.164189   10680 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1215 09:53:19.167345   10680 out.go:177]   - Using image kubernetesui/dashboard:v2.5.1
I1215 09:53:19.167874   10680 out.go:177]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1215 09:53:19.168926   10680 out.go:177]   - Using image kubernetesui/metrics-scraper:v1.0.7
I1215 09:53:19.168926   10680 addons.go:348] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1215 09:53:19.168926   10680 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1215 09:53:19.169973   10680 addons.go:348] installing /etc/kubernetes/addons/dashboard-ns.yaml
I1215 09:53:19.169973   10680 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I1215 09:53:19.173102   10680 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1215 09:53:19.173102   10680 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1215 09:53:19.200724   10680 start.go:786] CoreDNS already contains "host.minikube.internal" host record, skipping...
I1215 09:53:19.203936   10680 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1215 09:53:19.289610   10680 addons.go:348] installing /etc/kubernetes/addons/storageclass.yaml
I1215 09:53:19.289610   10680 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1215 09:53:19.292814   10680 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1215 09:53:19.304887   10680 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50767 SSHKeyPath:C:\Users\pesca\.minikube\machines\minikube\id_rsa Username:docker}
I1215 09:53:19.317008   10680 api_server.go:51] waiting for apiserver process to appear ...
I1215 09:53:19.317008   10680 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50767 SSHKeyPath:C:\Users\pesca\.minikube\machines\minikube\id_rsa Username:docker}
I1215 09:53:19.321303   10680 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1215 09:53:19.329727   10680 api_server.go:71] duration metric: took 307.5725ms to wait for apiserver process to appear ...
I1215 09:53:19.329727   10680 api_server.go:87] waiting for apiserver healthz status ...
I1215 09:53:19.329727   10680 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:50766/healthz ...
I1215 09:53:19.334408   10680 api_server.go:266] https://127.0.0.1:50766/healthz returned 200:
ok
I1215 09:53:19.335518   10680 api_server.go:140] control plane version: v1.23.6
I1215 09:53:19.335518   10680 api_server.go:130] duration metric: took 5.7909ms to wait for apiserver health ...
I1215 09:53:19.335518   10680 system_pods.go:43] waiting for kube-system pods to appear ...
I1215 09:53:19.339744   10680 system_pods.go:59] 7 kube-system pods found
I1215 09:53:19.339744   10680 system_pods.go:61] "coredns-64897985d-2t96b" [32e81d98-4c0e-46ec-9329-ebda5ba6b5cc] Running
I1215 09:53:19.339744   10680 system_pods.go:61] "etcd-minikube" [a7cd0a9c-2945-4281-89c2-e71a8e0e9fd3] Running
I1215 09:53:19.339744   10680 system_pods.go:61] "kube-apiserver-minikube" [96528e10-3258-4c28-b5e5-8794dea3e3c2] Running
I1215 09:53:19.339744   10680 system_pods.go:61] "kube-controller-manager-minikube" [78dcd395-4580-4a7f-9063-0254ec6a43ce] Running
I1215 09:53:19.339744   10680 system_pods.go:61] "kube-proxy-fgd5f" [a4c5a07f-374d-452e-b646-47f5a8ceb761] Running
I1215 09:53:19.339744   10680 system_pods.go:61] "kube-scheduler-minikube" [58ad3b0c-db5b-423c-b2a3-39f2e3e46da4] Running
I1215 09:53:19.339744   10680 system_pods.go:61] "storage-provisioner" [959b81f3-1586-4c43-8f0e-83e2f39864dd] Running
I1215 09:53:19.339744   10680 system_pods.go:74] duration metric: took 4.2256ms to wait for pod list to return data ...
I1215 09:53:19.339744   10680 kubeadm.go:572] duration metric: took 317.589ms to wait for : map[apiserver:true system_pods:true] ...
I1215 09:53:19.339744   10680 node_conditions.go:102] verifying NodePressure condition ...
I1215 09:53:19.342909   10680 node_conditions.go:122] node storage ephemeral capacity is 263174212Ki
I1215 09:53:19.342909   10680 node_conditions.go:123] node cpu capacity is 16
I1215 09:53:19.342909   10680 node_conditions.go:105] duration metric: took 3.165ms to run NodePressure ...
I1215 09:53:19.342909   10680 start.go:213] waiting for startup goroutines ...
I1215 09:53:19.366615   10680 addons.go:348] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I1215 09:53:19.366615   10680 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I1215 09:53:19.377145   10680 addons.go:348] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I1215 09:53:19.377145   10680 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I1215 09:53:19.387206   10680 addons.go:348] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I1215 09:53:19.387206   10680 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I1215 09:53:19.397224   10680 addons.go:348] installing /etc/kubernetes/addons/dashboard-dp.yaml
I1215 09:53:19.397224   10680 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4278 bytes)
I1215 09:53:19.408351   10680 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50767 SSHKeyPath:C:\Users\pesca\.minikube\machines\minikube\id_rsa Username:docker}
I1215 09:53:19.408867   10680 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.23.6/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1215 09:53:19.408867   10680 addons.go:348] installing /etc/kubernetes/addons/dashboard-role.yaml
I1215 09:53:19.408867   10680 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I1215 09:53:19.419923   10680 addons.go:348] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I1215 09:53:19.419923   10680 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I1215 09:53:19.432046   10680 addons.go:348] installing /etc/kubernetes/addons/dashboard-sa.yaml
I1215 09:53:19.432046   10680 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I1215 09:53:19.442676   10680 addons.go:348] installing /etc/kubernetes/addons/dashboard-secret.yaml
I1215 09:53:19.442676   10680 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I1215 09:53:19.468100   10680 addons.go:348] installing /etc/kubernetes/addons/dashboard-svc.yaml
I1215 09:53:19.468100   10680 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I1215 09:53:19.483492   10680 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.23.6/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I1215 09:53:19.510889   10680 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.23.6/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1215 09:53:19.683388   10680 out.go:177] * Enabled addons: storage-provisioner, dashboard, default-storageclass
I1215 09:53:19.683908   10680 addons.go:417] enableAddons completed in 661.7537ms
I1215 09:53:19.752995   10680 start.go:504] kubectl: 1.23.6, cluster: 1.23.6 (minor skew: 0)
I1215 09:53:19.754048   10680 out.go:177] * Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* -- Logs begin at Thu 2022-12-15 08:15:36 UTC, end at Thu 2022-12-15 10:13:44 UTC. --
Dec 15 09:33:46 minikube dockerd[254]: time="2022-12-15T09:33:46.814439791Z" level=info msg="Attempting next endpoint for pull after error: manifest unknown: manifest unknown"
Dec 15 09:33:48 minikube dockerd[254]: time="2022-12-15T09:33:48.711342595Z" level=info msg="Attempting next endpoint for pull after error: manifest unknown: manifest unknown"
Dec 15 09:34:03 minikube dockerd[254]: time="2022-12-15T09:34:03.291470895Z" level=info msg="Attempting next endpoint for pull after error: manifest unknown: manifest unknown"
Dec 15 09:34:05 minikube dockerd[254]: time="2022-12-15T09:34:05.225446000Z" level=info msg="Attempting next endpoint for pull after error: manifest unknown: manifest unknown"
Dec 15 09:34:14 minikube dockerd[254]: time="2022-12-15T09:34:14.278364295Z" level=info msg="ignoring event" container=cf3091f5f3d0affba8a126b3678d22fd678f158f97b1f43f8a43a6fda8fb00b9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:34:14 minikube dockerd[254]: time="2022-12-15T09:34:14.279918079Z" level=info msg="ignoring event" container=551872dbcbfe0e38ff4b2076ef0cf766dabea734253865e9d58bb24f392ff729 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:34:23 minikube dockerd[254]: time="2022-12-15T09:34:23.411881094Z" level=info msg="ignoring event" container=3dc6e0030102facad530a31b15543ce9eea8554c7a2de0e8f14852be8b5a3973 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:34:23 minikube dockerd[254]: time="2022-12-15T09:34:23.470382646Z" level=info msg="ignoring event" container=b22e8a1a78bedcb33f97e7f12afcb7c3d5f3724b684283000b1d41f783aaa933 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:34:24 minikube dockerd[254]: time="2022-12-15T09:34:24.587670765Z" level=info msg="ignoring event" container=c4d7f6af3b89a11d3289cdf46aee326878cb13c23903093ebbae6d6a2e59510d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:34:24 minikube dockerd[254]: time="2022-12-15T09:34:24.588545607Z" level=info msg="ignoring event" container=1f3ab4c2b4371717b3d138e93e5f2ad5665b91fa446e7e923abaf179498932bf module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:34:39 minikube dockerd[254]: time="2022-12-15T09:34:39.480869850Z" level=info msg="ignoring event" container=a995f0489942421ad06de6d42521afdad454220d94346d4fe5032e07d8236427 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:34:41 minikube dockerd[254]: time="2022-12-15T09:34:41.676678506Z" level=info msg="ignoring event" container=52fd505915db11bab2867dd1cdb070ad3a507cb078c0159119810ccfc164cf91 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:35:02 minikube dockerd[254]: time="2022-12-15T09:35:02.880698964Z" level=info msg="ignoring event" container=8fa92bf6004ef95515ed6a330e21a1b55785e7c4f0f2a56c0989068bb976dfb3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:35:03 minikube dockerd[254]: time="2022-12-15T09:35:03.065750420Z" level=info msg="ignoring event" container=625068a3713e94b58f7764a81d1801b1286361eea14e6daee27f458b17e7981a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:35:12 minikube dockerd[254]: time="2022-12-15T09:35:12.790151712Z" level=info msg="ignoring event" container=cb4f89ff5f7bb4aed6b7e3517255f652024e50a4a4d92b9adf3cf23d598d154f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:35:12 minikube dockerd[254]: time="2022-12-15T09:35:12.994241600Z" level=info msg="ignoring event" container=1f8ba3adaac5e3630abd004953cbf663593b778f16064b863771b1fac9aa1032 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:35:30 minikube dockerd[254]: time="2022-12-15T09:35:30.480725520Z" level=info msg="ignoring event" container=fc846c8196d5f4d182e240b91655a956af24723a19ad249a1dfa2dd83e0d76ff module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:35:34 minikube dockerd[254]: time="2022-12-15T09:35:34.291026118Z" level=info msg="ignoring event" container=9d8b822c48b22dcc6a62b0140e0e44e40a71f7e4e730e2019e75bd1d97f92905 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:35:35 minikube dockerd[254]: time="2022-12-15T09:35:35.300434390Z" level=info msg="ignoring event" container=61770b39e52b47e98dab52c78afb46f7db0616be3a68e70218497aef57d38d58 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:35:50 minikube dockerd[254]: time="2022-12-15T09:35:50.501209111Z" level=info msg="ignoring event" container=c6db3153e9a238c67342e661c3b07a4522d24870af5f24713e79fabf65202be2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:35:59 minikube dockerd[254]: time="2022-12-15T09:35:59.460128812Z" level=info msg="ignoring event" container=35e9d24df3bd13e34f7aafe36ef37d5cc03524b415d8e7341cc63b56cc2f1f51 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:36:16 minikube dockerd[254]: time="2022-12-15T09:36:16.452551655Z" level=info msg="ignoring event" container=f8c3d4d7e2fe01b089bba0b04ac496ebf91e6920284ac912914654f77e9877eb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:36:48 minikube dockerd[254]: time="2022-12-15T09:36:48.512816366Z" level=info msg="ignoring event" container=fd2d4ae32da836e95cca7d2f9cd589644bf5d33e6996baaeea0dc829d45a954f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:36:57 minikube dockerd[254]: time="2022-12-15T09:36:57.459730447Z" level=info msg="ignoring event" container=de89d955ac157bcf3ff21e257aac9b6b7e66a44d4e8e0324f24e41bf780148cf module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:38:06 minikube dockerd[254]: time="2022-12-15T09:38:06.076240279Z" level=info msg="ignoring event" container=64d46e78e5e15dd5a4ca1b123e5ff7b97e178c4c8b944d5971b4f724931ca965 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:38:06 minikube dockerd[254]: time="2022-12-15T09:38:06.079501334Z" level=info msg="ignoring event" container=1c05283c7c982e8f44aaa563b1dd953a9675cfa39172df35d5f7b09214d26ce2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:38:16 minikube dockerd[254]: time="2022-12-15T09:38:16.240648578Z" level=info msg="ignoring event" container=33b143346f21d5a8b3e8a0c195d811ccad7a7b9208bb4cca13a11498cff39e74 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:38:17 minikube dockerd[254]: time="2022-12-15T09:38:17.412730581Z" level=info msg="ignoring event" container=dd745b857f758ea9e84fbbd3764ebd9130290baaa22fc8c85e3924a59ad0ca33 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:38:33 minikube dockerd[254]: time="2022-12-15T09:38:33.469470982Z" level=info msg="ignoring event" container=16eaef6d3f3b1778bdab8af376ebed6db0a7351ba1dddc03a6e8f70569086901 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:39:00 minikube dockerd[254]: time="2022-12-15T09:39:00.430839089Z" level=info msg="ignoring event" container=50f6df1147b5454fafa24f95d195702e16c9a2fe448b771027d60e40118c2dda module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:39:41 minikube dockerd[254]: time="2022-12-15T09:39:41.450839379Z" level=info msg="ignoring event" container=a7f6e3bee5c52bb4eb93645d5dbc1b19e6a482e310fad69e24a23307bc680290 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:41:16 minikube dockerd[254]: time="2022-12-15T09:41:16.451444294Z" level=info msg="ignoring event" container=59b474e3d114ad9e5621287ff0ab846b69e586658b29d1117baf1d3071c0c400 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:44:07 minikube dockerd[254]: time="2022-12-15T09:44:07.480545771Z" level=info msg="ignoring event" container=8122e28b892345d88cc01750ee28b1d01d18fbe0bad853e539a54e120fd1f645 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:49:09 minikube dockerd[254]: time="2022-12-15T09:49:09.451249929Z" level=info msg="ignoring event" container=f73cf81ba6b42b018a1f0a71a719ddfe195a423d345eb0791685efbb82cebf8f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:51:41 minikube dockerd[254]: time="2022-12-15T09:51:41.678881227Z" level=info msg="ignoring event" container=0f0feccd8ccffda36149cb55d9c721ca505527a1ffe6663831ff69f61ca6178f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:54:03 minikube dockerd[254]: time="2022-12-15T09:54:03.921151402Z" level=info msg="ignoring event" container=cbc127299b85959efb66ec0e3ca00a1e392168fac35b78e8fe982c4e2e0437c8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:54:04 minikube dockerd[254]: time="2022-12-15T09:54:04.359369991Z" level=info msg="ignoring event" container=56169e119c685933d8ac137a3679621f65fc42ed414b78f099e4656a4b941dde module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:54:21 minikube dockerd[254]: time="2022-12-15T09:54:21.440394877Z" level=info msg="ignoring event" container=af68e20e1f2d123a3e5714c297a32cbcb261020a4b0b8ad653c587d410f48438 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:54:52 minikube dockerd[254]: time="2022-12-15T09:54:52.441593953Z" level=info msg="ignoring event" container=25a8143fa5eda3e4f2619da3fcb57504a8903a8174a6a3db0e43832ba39243f5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:55:47 minikube dockerd[254]: time="2022-12-15T09:55:47.470554069Z" level=info msg="ignoring event" container=00bdc21b18b43e22be906406c7bec7a49b303c6e6b80764c909077f2da149f46 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 09:57:18 minikube dockerd[254]: time="2022-12-15T09:57:18.468506862Z" level=info msg="ignoring event" container=45e29c411fa375e1aa0eec50010d90742ce1b724a99ffb70181108e9a032666f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 10:00:04 minikube dockerd[254]: time="2022-12-15T10:00:04.440483568Z" level=info msg="ignoring event" container=bf70369e4ab42fa749a680c82d418eb7984a9e8d6d66809d475127f40d18d1ee module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 10:02:32 minikube dockerd[254]: time="2022-12-15T10:02:32.675171263Z" level=info msg="ignoring event" container=86f718cdd126538c3da007e0a7213a21f25e7ca6875cab210265006fac792805 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 10:04:07 minikube dockerd[254]: time="2022-12-15T10:04:07.790772818Z" level=info msg="ignoring event" container=8b6f5e9ca3ebaef42ad7254d943e34bc01e05210b6a96f99fa44e5418fe52ce8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 10:04:08 minikube dockerd[254]: time="2022-12-15T10:04:08.790642137Z" level=info msg="ignoring event" container=d7b6d0ba9a54380c87a38d72b0b269565c8aa457ed2c7190b8054403f32ead5b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 10:04:23 minikube dockerd[254]: time="2022-12-15T10:04:23.449672438Z" level=info msg="ignoring event" container=9855f0af979d6cf14cd052972f7db676c07bfd0891168d8d97901c122a1b292a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 10:04:53 minikube dockerd[254]: time="2022-12-15T10:04:53.459117354Z" level=info msg="ignoring event" container=e5c5eb8a4373ca2df7eae61ee9cc7377b84cd63d12706058cac26e100d2d7e30 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 10:05:46 minikube dockerd[254]: time="2022-12-15T10:05:46.492008047Z" level=info msg="ignoring event" container=08043039b5ec66b66c966b477ea50f670600984a10cbc501257d96fc7cb72000 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 10:07:20 minikube dockerd[254]: time="2022-12-15T10:07:20.481622388Z" level=info msg="ignoring event" container=1ed12d2fcfbd43d8571a5eaaffb4ed17e49df4b991d2b247e1811f297b006260 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 10:07:21 minikube dockerd[254]: time="2022-12-15T10:07:21.785181524Z" level=info msg="ignoring event" container=d23bb69c8d98b147ee7cdcde8c38edefdda4bc7851f0e6e681189db85700465d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 10:07:35 minikube dockerd[254]: time="2022-12-15T10:07:35.891041815Z" level=info msg="ignoring event" container=f63964512b1c8fc83c8daa1836fbb3e93c1efba5efde5526b32673c4eb4a60aa module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 10:07:36 minikube dockerd[254]: time="2022-12-15T10:07:36.939426565Z" level=info msg="ignoring event" container=4ec55d86af17beaa66219a20c3ff9f424c94d1cdc2aba7e5eded5996dd10765e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 10:07:54 minikube dockerd[254]: time="2022-12-15T10:07:54.480063050Z" level=info msg="ignoring event" container=18f56a07937aaef2d183495139dd9050f70143856ce4ca53293eac5dbb11578d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 10:08:03 minikube dockerd[254]: time="2022-12-15T10:08:03.680226411Z" level=info msg="ignoring event" container=ea1d4a863c22c356b98417679d4e5e02bfdcc9dce43419f4e9498a127edc10e5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 10:08:23 minikube dockerd[254]: time="2022-12-15T10:08:23.020476039Z" level=info msg="ignoring event" container=21b3c1b71f7007cd0ea7eb09cd59148d0dcff6b4e10ae9a93bc7f384e726dfeb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 10:08:23 minikube dockerd[254]: time="2022-12-15T10:08:23.133667280Z" level=info msg="ignoring event" container=a945864898f0537b9030934173bdd6eea37da39ba8deb9050ff9e9ad9cc9e8ae module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 10:08:38 minikube dockerd[254]: time="2022-12-15T10:08:38.451501641Z" level=info msg="ignoring event" container=9d5d7b071fbd7b1cc37eb5a9dc99d5a0b120cfd4825a39b95cba65f2102712be module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 10:09:04 minikube dockerd[254]: time="2022-12-15T10:09:04.450838485Z" level=info msg="ignoring event" container=969f3caf238c5423e234c45d8a72d4f74d03831b6109f190ae74986112961e53 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 10:09:48 minikube dockerd[254]: time="2022-12-15T10:09:48.459937577Z" level=info msg="ignoring event" container=109dddfc47efd55a5e4332e314cd9e2dc25d2e4bc83df33851a25fc4a55292b2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 15 10:11:12 minikube dockerd[254]: time="2022-12-15T10:11:12.460348889Z" level=info msg="ignoring event" container=f446c077de8116bc04bd83818e5b3821d893bd669e2c75806bf3e98afa5a493b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                  CREATED             STATE               NAME                        ATTEMPT             POD ID
f446c077de811       51cdee7b4d50f                                                                                          2 minutes ago       Exited              api                         5                   2ae9560171486
c6537b1d78ad4       kubernetesui/dashboard@sha256:cc746e7a0b1eec0db01cbabbb6386b23d7af97e79fa9e36bb883a95b7eb96fe2         2 hours ago         Running             kubernetes-dashboard        0                   f5ff79cd240f1
16111b401384b       kubernetesui/metrics-scraper@sha256:36d5b3f60e1a144cc5ada820910535074bdf5cf73fb70d1ff1681537eef4e172   2 hours ago         Running             dashboard-metrics-scraper   0                   66798739f754c
c0422f17b1274       6e38f40d628db                                                                                          2 hours ago         Running             storage-provisioner         3                   bc048a11edce7
94153bf83cc5c       a4ca41631cc7a                                                                                          2 hours ago         Running             coredns                     1                   ee2f0e01a9a79
66f8efe90cc96       4c03754524064                                                                                          2 hours ago         Running             kube-proxy                  1                   caf3c0e676455
a066295e0cb73       6e38f40d628db                                                                                          2 hours ago         Exited              storage-provisioner         2                   bc048a11edce7
536e9f8b5858e       25f8c7f3da61c                                                                                          2 hours ago         Running             etcd                        1                   8998bd9cace31
304e517aff5fc       595f327f224a4                                                                                          2 hours ago         Running             kube-scheduler              1                   49ba0ee6c6a99
43b305667b9c5       df7b72818ad2e                                                                                          2 hours ago         Running             kube-controller-manager     1                   5f553d5edb146
7319aeb72e88f       8fa62c12256df                                                                                          2 hours ago         Running             kube-apiserver              1                   b7226e5e2d259
dcfe239345f64       a4ca41631cc7a                                                                                          2 hours ago         Exited              coredns                     0                   d7ff670d46cbe
5be6d078984f0       4c03754524064                                                                                          2 hours ago         Exited              kube-proxy                  0                   2a6543437dd33
3eb61093274cb       25f8c7f3da61c                                                                                          2 hours ago         Exited              etcd                        0                   60c979d103a4e
84d1ba317f9b6       8fa62c12256df                                                                                          2 hours ago         Exited              kube-apiserver              0                   88bb836361493
e593bca41b920       df7b72818ad2e                                                                                          2 hours ago         Exited              kube-controller-manager     0                   68523d3920994
07c0346d96003       595f327f224a4                                                                                          2 hours ago         Exited              kube-scheduler              0                   09ffd57f765b3

* 
* ==> coredns [94153bf83cc5] <==
* [INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration MD5 = c23ed519c17e71ee396ed052e6209e94
CoreDNS-1.8.6
linux/amd64, go1.17.1, 13a9191
[INFO] plugin/ready: Still waiting on: "kubernetes"
[ERROR] plugin/errors: 2 8375368103688918435.4175056028804337543. HINFO: read udp 172.17.0.2:52585->192.168.65.2:53: i/o timeout
[ERROR] plugin/errors: 2 8375368103688918435.4175056028804337543. HINFO: read udp 172.17.0.2:46304->192.168.65.2:53: i/o timeout
[INFO] plugin/ready: Still waiting on: "kubernetes"

* 
* ==> coredns [dcfe239345f6] <==
* [INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration MD5 = c23ed519c17e71ee396ed052e6209e94
CoreDNS-1.8.6
linux/amd64, go1.17.1, 13a9191
[INFO] plugin/ready: Still waiting on: "kubernetes"
[ERROR] plugin/errors: 2 1343268982219808349.782049949332080417. HINFO: read udp 172.17.0.2:40550->192.168.65.2:53: i/o timeout
[ERROR] plugin/errors: 2 1343268982219808349.782049949332080417. HINFO: read udp 172.17.0.2:38532->192.168.65.2:53: i/o timeout
[ERROR] plugin/errors: 2 1343268982219808349.782049949332080417. HINFO: read udp 172.17.0.2:53730->192.168.65.2:53: i/o timeout
[INFO] plugin/ready: Still waiting on: "kubernetes"

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane,master
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=f0a6d95bdb2f2b83c4f952383fe29de03c269eab
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2022_12_15T09_10_24_0700
                    minikube.k8s.io/version=v1.26.0-beta.1
                    node-role.kubernetes.io/control-plane=
                    node-role.kubernetes.io/master=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 15 Dec 2022 08:10:23 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Thu, 15 Dec 2022 10:13:42 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 15 Dec 2022 10:12:53 +0000   Thu, 15 Dec 2022 08:10:23 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 15 Dec 2022 10:12:53 +0000   Thu, 15 Dec 2022 08:10:23 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 15 Dec 2022 10:12:53 +0000   Thu, 15 Dec 2022 08:10:23 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 15 Dec 2022 10:12:53 +0000   Thu, 15 Dec 2022 08:10:35 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                16
  ephemeral-storage:  263174212Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             24548208Ki
  pods:               110
Allocatable:
  cpu:                16
  ephemeral-storage:  263174212Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             24548208Ki
  pods:               110
System Info:
  Machine ID:                 1729fd8b7c184ebda96a08181510f608
  System UUID:                1729fd8b7c184ebda96a08181510f608
  Boot ID:                    fe8b5f32-84e8-4be8-a49f-1e14ee121ead
  Kernel Version:             5.10.16.3-microsoft-standard-WSL2
  OS Image:                   Ubuntu 20.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.15
  Kubelet Version:            v1.23.6
  Kube-Proxy Version:         v1.23.6
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     foip-deployment-685fd58db6-6d8l2             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5m22s
  kube-system                 coredns-64897985d-2t96b                      100m (0%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (0%!)(MISSING)     123m
  kube-system                 etcd-minikube                                100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (0%!)(MISSING)       0 (0%!)(MISSING)         123m
  kube-system                 kube-apiserver-minikube                      250m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         123m
  kube-system                 kube-controller-manager-minikube             200m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         123m
  kube-system                 kube-proxy-fgd5f                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         123m
  kube-system                 kube-scheduler-minikube                      100m (0%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         123m
  kube-system                 storage-provisioner                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         123m
  kubernetes-dashboard        dashboard-metrics-scraper-58549894f-g842z    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         101m
  kubernetes-dashboard        kubernetes-dashboard-8469778f77-6mgkc        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         101m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (4%!)(MISSING)   0 (0%!)(MISSING)
  memory             170Mi (0%!)(MISSING)  170Mi (0%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:              <none>

* 
* ==> dmesg <==
* [  +0.000235] FS-Cache: N-key=[10] '34323934393734303532'
[  +0.000454] init: (1) ERROR: ConfigApplyWindowsLibPath:2431: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000002]  failed 2
[  +0.010714] FS-Cache: Duplicate cookie detected
[  +0.000440] FS-Cache: O-cookie c=00000000fcb6b924 [p=00000000cafb9aca fl=222 nc=0 na=1]
[  +0.000360] FS-Cache: O-cookie d=0000000010e0dd63 n=00000000c702c42d
[  +0.000252] FS-Cache: O-key=[10] '34323934393734303533'
[  +0.000200] FS-Cache: N-cookie c=00000000cf06e329 [p=00000000cafb9aca fl=2 nc=0 na=1]
[  +0.000362] FS-Cache: N-cookie d=0000000010e0dd63 n=0000000045a9803a
[  +0.000332] FS-Cache: N-key=[10] '34323934393734303533'
[  +0.001565] FS-Cache: Duplicate cookie detected
[  +0.000267] FS-Cache: O-cookie c=00000000fcb6b924 [p=00000000cafb9aca fl=222 nc=0 na=1]
[  +0.000526] FS-Cache: O-cookie d=0000000010e0dd63 n=00000000c702c42d
[  +0.000302] FS-Cache: O-key=[10] '34323934393734303533'
[  +0.000217] FS-Cache: N-cookie c=00000000cf06e329 [p=00000000cafb9aca fl=2 nc=0 na=1]
[  +0.000285] FS-Cache: N-cookie d=0000000010e0dd63 n=00000000ed8658e4
[  +0.000264] FS-Cache: N-key=[10] '34323934393734303533'
[  +0.020158] WARNING: /usr/share/zoneinfo/Europe/Madrid not found. Is the tzdata package installed?
[  +0.090076] init: (1) ERROR: ConfigApplyWindowsLibPath:2431: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000002]  failed 2
[  +0.008311] FS-Cache: Duplicate cookie detected
[  +0.000360] FS-Cache: O-cookie c=0000000055a3e6f2 [p=00000000cafb9aca fl=222 nc=0 na=1]
[  +0.000408] FS-Cache: O-cookie d=0000000010e0dd63 n=000000009a2a9a0a
[  +0.000245] FS-Cache: O-key=[10] '34323934393734303635'
[  +0.000218] FS-Cache: N-cookie c=00000000d92e6a2d [p=00000000cafb9aca fl=2 nc=0 na=1]
[  +0.000281] FS-Cache: N-cookie d=0000000010e0dd63 n=000000004a439b60
[  +0.000378] FS-Cache: N-key=[10] '34323934393734303635'
[  +0.003456] FS-Cache: Duplicate cookie detected
[  +0.000484] FS-Cache: O-cookie c=00000000d92e6a2d [p=00000000cafb9aca fl=222 nc=0 na=1]
[  +0.000390] FS-Cache: O-cookie d=0000000010e0dd63 n=00000000ea61d730
[  +0.000364] FS-Cache: O-key=[10] '34323934393734303636'
[  +0.000262] FS-Cache: N-cookie c=000000006b637614 [p=00000000cafb9aca fl=2 nc=0 na=1]
[  +0.000465] FS-Cache: N-cookie d=0000000010e0dd63 n=0000000057c00df2
[  +0.000353] FS-Cache: N-key=[10] '34323934393734303636'
[  +0.004051] WARNING: /usr/share/zoneinfo/Europe/Madrid not found. Is the tzdata package installed?
[  +0.087011] FS-Cache: Duplicate cookie detected
[  +0.000421] FS-Cache: O-cookie c=00000000df1480eb [p=00000000cafb9aca fl=222 nc=0 na=1]
[  +0.000446] FS-Cache: O-cookie d=0000000010e0dd63 n=00000000d2e35ba2
[  +0.000363] FS-Cache: O-key=[10] '34323934393734303735'
[  +0.000249] FS-Cache: N-cookie c=000000009062ba9f [p=00000000cafb9aca fl=2 nc=0 na=1]
[  +0.000414] FS-Cache: N-cookie d=0000000010e0dd63 n=00000000b4844179
[  +0.000393] FS-Cache: N-key=[10] '34323934393734303735'
[  +0.000750] init: (1) ERROR: ConfigApplyWindowsLibPath:2431: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000003]  failed 2
[  +0.001145] init: (2) ERROR: UtilCreateProcessAndWait:702: /bin/mount failed with 2
[  +0.000674] init: (1) ERROR: UtilCreateProcessAndWait:722: /bin/mount failed with status 0x
[  +0.000002] ff00
[  +0.000483] init: (1) ERROR: ConfigMountFsTab:2484: Processing fstab with mount -a failed.
[  +0.002972] FS-Cache: Duplicate cookie detected
[  +0.000540] FS-Cache: O-cookie c=000000009062ba9f [p=00000000cafb9aca fl=222 nc=0 na=1]
[  +0.000451] FS-Cache: O-cookie d=0000000010e0dd63 n=000000006b1b8efc
[  +0.000353] FS-Cache: O-key=[10] '34323934393734303736'
[  +0.000307] FS-Cache: N-cookie c=00000000a9af146e [p=00000000cafb9aca fl=2 nc=0 na=1]
[  +0.000441] FS-Cache: N-cookie d=0000000010e0dd63 n=000000005354710d
[  +0.000363] FS-Cache: N-key=[10] '34323934393734303736'
[  +0.011369] WARNING: /usr/share/zoneinfo/Europe/Madrid not found. Is the tzdata package installed?
[  +0.005270] init: (8) ERROR: CreateProcessEntryCommon:443: getpwuid(0) failed 2
[  +0.000614] init: (8) ERROR: CreateProcessEntryCommon:446: getpwuid(0) failed 2
[Dec15 10:04] init: (206) ERROR: LogException:33: TELEMETRY: (null) No such file or directory @d:\os\src\onecore\vm\linux\mountutil\mountutilcpp.h:19 (MountEnum)


* 
* ==> etcd [3eb61093274c] <==
* {"level":"info","ts":"2022-12-15T08:10:19.779Z","caller":"etcdmain/etcd.go:72","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2022-12-15T08:10:19.779Z","caller":"embed/etcd.go:131","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2022-12-15T08:10:19.779Z","caller":"embed/etcd.go:478","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2022-12-15T08:10:19.780Z","caller":"embed/etcd.go:139","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2022-12-15T08:10:19.780Z","caller":"embed/etcd.go:307","msg":"starting an etcd server","etcd-version":"3.5.1","git-sha":"e8732fb5f","go-version":"go1.16.3","go-os":"linux","go-arch":"amd64","max-cpu-set":16,"max-cpu-available":16,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.49.2:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-size-bytes":2147483648,"pre-vote":true,"initial-corrupt-check":false,"corrupt-check-time-interval":"0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2022-12-15T08:10:19.783Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"2.785109ms"}
{"level":"info","ts":"2022-12-15T08:10:19.871Z","caller":"etcdserver/raft.go:448","msg":"starting local member","local-member-id":"aec36adc501070cc","cluster-id":"fa54960ea34d58be"}
{"level":"info","ts":"2022-12-15T08:10:19.871Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2022-12-15T08:10:19.871Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 0"}
{"level":"info","ts":"2022-12-15T08:10:19.871Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2022-12-15T08:10:19.871Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 1"}
{"level":"info","ts":"2022-12-15T08:10:19.872Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"warn","ts":"2022-12-15T08:10:19.875Z","caller":"auth/store.go:1220","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2022-12-15T08:10:19.878Z","caller":"mvcc/kvstore.go:415","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2022-12-15T08:10:19.880Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2022-12-15T08:10:19.882Z","caller":"etcdserver/server.go:843","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.1","cluster-version":"to_be_decided"}
{"level":"info","ts":"2022-12-15T08:10:19.882Z","caller":"etcdserver/server.go:728","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2022-12-15T08:10:19.882Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2022-12-15T08:10:19.882Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2022-12-15T08:10:19.883Z","caller":"embed/etcd.go:687","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2022-12-15T08:10:19.883Z","caller":"embed/etcd.go:580","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2022-12-15T08:10:19.883Z","caller":"embed/etcd.go:552","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2022-12-15T08:10:19.883Z","caller":"embed/etcd.go:276","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2022-12-15T08:10:19.883Z","caller":"embed/etcd.go:762","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2022-12-15T08:10:20.673Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2022-12-15T08:10:20.673Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2022-12-15T08:10:20.673Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2022-12-15T08:10:20.673Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2022-12-15T08:10:20.673Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2022-12-15T08:10:20.673Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2022-12-15T08:10:20.673Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2022-12-15T08:10:20.673Z","caller":"etcdserver/server.go:2476","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2022-12-15T08:10:20.674Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2022-12-15T08:10:20.674Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2022-12-15T08:10:20.674Z","caller":"etcdserver/server.go:2500","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2022-12-15T08:10:20.674Z","caller":"etcdserver/server.go:2027","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2022-12-15T08:10:20.674Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2022-12-15T08:10:20.674Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2022-12-15T08:10:20.674Z","caller":"etcdmain/main.go:47","msg":"notifying init daemon"}
{"level":"info","ts":"2022-12-15T08:10:20.674Z","caller":"etcdmain/main.go:53","msg":"successfully notified init daemon"}
{"level":"info","ts":"2022-12-15T08:10:20.675Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2022-12-15T08:10:20.675Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"192.168.49.2:2379"}
{"level":"info","ts":"2022-12-15T08:14:01.134Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2022-12-15T08:14:01.135Z","caller":"embed/etcd.go:367","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> etcd [536e9f8b5858] <==
* {"level":"info","ts":"2022-12-15T08:15:49.274Z","caller":"embed/etcd.go:276","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2022-12-15T08:15:49.274Z","caller":"embed/etcd.go:762","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2022-12-15T08:15:50.209Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 2"}
{"level":"info","ts":"2022-12-15T08:15:50.209Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 2"}
{"level":"info","ts":"2022-12-15T08:15:50.209Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2022-12-15T08:15:50.209Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 3"}
{"level":"info","ts":"2022-12-15T08:15:50.209Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2022-12-15T08:15:50.209Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 3"}
{"level":"info","ts":"2022-12-15T08:15:50.209Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 3"}
{"level":"info","ts":"2022-12-15T08:15:50.221Z","caller":"etcdserver/server.go:2027","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2022-12-15T08:15:50.221Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2022-12-15T08:15:50.221Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2022-12-15T08:15:50.222Z","caller":"etcdmain/main.go:47","msg":"notifying init daemon"}
{"level":"info","ts":"2022-12-15T08:15:50.222Z","caller":"etcdmain/main.go:53","msg":"successfully notified init daemon"}
{"level":"info","ts":"2022-12-15T08:15:50.223Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2022-12-15T08:15:50.223Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"192.168.49.2:2379"}
{"level":"info","ts":"2022-12-15T08:25:50.248Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":882}
{"level":"info","ts":"2022-12-15T08:25:50.248Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":882,"took":"596.573s"}
{"level":"info","ts":"2022-12-15T08:30:50.264Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1092}
{"level":"info","ts":"2022-12-15T08:30:50.264Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":1092,"took":"382.246s"}
{"level":"info","ts":"2022-12-15T08:35:50.274Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1303}
{"level":"info","ts":"2022-12-15T08:35:50.274Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":1303,"took":"322.253s"}
{"level":"info","ts":"2022-12-15T08:40:50.286Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1596}
{"level":"info","ts":"2022-12-15T08:40:50.287Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":1596,"took":"318.044s"}
{"level":"info","ts":"2022-12-15T08:45:50.298Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1959}
{"level":"info","ts":"2022-12-15T08:45:50.298Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":1959,"took":"519.898s"}
{"level":"info","ts":"2022-12-15T08:50:50.313Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2192}
{"level":"info","ts":"2022-12-15T08:50:50.314Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":2192,"took":"350.706s"}
{"level":"info","ts":"2022-12-15T08:55:50.318Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2434}
{"level":"info","ts":"2022-12-15T08:55:50.319Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":2434,"took":"302.675s"}
{"level":"info","ts":"2022-12-15T09:00:50.332Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2723}
{"level":"info","ts":"2022-12-15T09:00:50.333Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":2723,"took":"365.735s"}
{"level":"info","ts":"2022-12-15T09:05:50.341Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3022}
{"level":"info","ts":"2022-12-15T09:05:50.342Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":3022,"took":"416.601s"}
{"level":"info","ts":"2022-12-15T09:10:50.356Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3489}
{"level":"info","ts":"2022-12-15T09:10:50.357Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":3489,"took":"544.254s"}
{"level":"info","ts":"2022-12-15T09:15:50.371Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3794}
{"level":"info","ts":"2022-12-15T09:15:50.372Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":3794,"took":"409.869s"}
{"level":"info","ts":"2022-12-15T09:20:50.376Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4004}
{"level":"info","ts":"2022-12-15T09:20:50.377Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":4004,"took":"353.652s"}
{"level":"info","ts":"2022-12-15T09:25:50.411Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4252}
{"level":"info","ts":"2022-12-15T09:25:50.413Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":4252,"took":"1.242539ms"}
{"level":"info","ts":"2022-12-15T09:30:50.440Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4462}
{"level":"info","ts":"2022-12-15T09:30:50.441Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":4462,"took":"436.429s"}
{"level":"info","ts":"2022-12-15T09:35:50.445Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4743}
{"level":"info","ts":"2022-12-15T09:35:50.446Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":4743,"took":"460.996s"}
{"level":"info","ts":"2022-12-15T09:40:50.451Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5305}
{"level":"info","ts":"2022-12-15T09:40:50.466Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":5305,"took":"14.472973ms"}
{"level":"info","ts":"2022-12-15T09:45:50.486Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5626}
{"level":"info","ts":"2022-12-15T09:45:50.487Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":5626,"took":"592.376s"}
{"level":"info","ts":"2022-12-15T09:50:50.500Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5845}
{"level":"info","ts":"2022-12-15T09:50:50.501Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":5845,"took":"437.311s"}
{"level":"info","ts":"2022-12-15T09:55:50.509Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6061}
{"level":"info","ts":"2022-12-15T09:55:50.510Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":6061,"took":"407.424s"}
{"level":"info","ts":"2022-12-15T10:00:50.524Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6331}
{"level":"info","ts":"2022-12-15T10:00:50.525Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":6331,"took":"435.347s"}
{"level":"info","ts":"2022-12-15T10:05:50.570Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6554}
{"level":"info","ts":"2022-12-15T10:05:50.570Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":6554,"took":"373.089s"}
{"level":"info","ts":"2022-12-15T10:10:50.592Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6829}
{"level":"info","ts":"2022-12-15T10:10:50.593Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":6829,"took":"422.232s"}

* 
* ==> kernel <==
*  10:13:44 up  2:05,  0 users,  load average: 0.05, 0.06, 0.07
Linux minikube 5.10.16.3-microsoft-standard-WSL2 #1 SMP Fri Apr 2 22:23:49 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 20.04.4 LTS"

* 
* ==> kube-apiserver [7319aeb72e88] <==
* I1215 08:15:50.911178       1 plugins.go:158] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I1215 08:15:50.911202       1 plugins.go:161] Loaded 11 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,CertificateSubjectRestriction,ValidatingAdmissionWebhook,ResourceQuota.
W1215 08:15:50.926854       1 genericapiserver.go:538] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I1215 08:15:51.544502       1 dynamic_cafile_content.go:156] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1215 08:15:51.544584       1 dynamic_serving_content.go:131] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I1215 08:15:51.544600       1 secure_serving.go:266] Serving securely on [::]:8443
I1215 08:15:51.544610       1 dynamic_cafile_content.go:156] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1215 08:15:51.544663       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1215 08:15:51.545022       1 controller.go:83] Starting OpenAPI AggregationController
I1215 08:15:51.545086       1 dynamic_serving_content.go:131] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1215 08:15:51.545262       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I1215 08:15:51.545295       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1215 08:15:51.545374       1 customresource_discovery_controller.go:209] Starting DiscoveryController
I1215 08:15:51.545467       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I1215 08:15:51.545501       1 shared_informer.go:240] Waiting for caches to sync for cluster_authentication_trust_controller
I1215 08:15:51.545664       1 dynamic_cafile_content.go:156] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1215 08:15:51.545730       1 controller.go:85] Starting OpenAPI controller
I1215 08:15:51.545769       1 naming_controller.go:291] Starting NamingConditionController
I1215 08:15:51.545787       1 establishing_controller.go:76] Starting EstablishingController
I1215 08:15:51.545817       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I1215 08:15:51.545827       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1215 08:15:51.545843       1 crd_finalizer.go:266] Starting CRDFinalizer
I1215 08:15:51.545970       1 available_controller.go:491] Starting AvailableConditionController
I1215 08:15:51.545991       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I1215 08:15:51.545977       1 dynamic_cafile_content.go:156] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1215 08:15:51.547702       1 apf_controller.go:317] Starting API Priority and Fairness config controller
I1215 08:15:51.551780       1 autoregister_controller.go:141] Starting autoregister controller
I1215 08:15:51.551812       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1215 08:15:51.551830       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I1215 08:15:51.551834       1 shared_informer.go:240] Waiting for caches to sync for crd-autoregister
E1215 08:15:51.554676       1 controller.go:157] Error removing old endpoints from kubernetes service: no master IPs were listed in storage, refusing to erase all endpoints for the kubernetes service
I1215 08:15:51.567305       1 controller.go:611] quota admission added evaluator for: leases.coordination.k8s.io
I1215 08:15:51.570100       1 shared_informer.go:247] Caches are synced for node_authorizer 
I1215 08:15:51.665239       1 shared_informer.go:247] Caches are synced for cluster_authentication_trust_controller 
I1215 08:15:51.665346       1 cache.go:39] Caches are synced for AvailableConditionController controller
I1215 08:15:51.665464       1 shared_informer.go:247] Caches are synced for crd-autoregister 
I1215 08:15:51.665563       1 cache.go:39] Caches are synced for autoregister controller
I1215 08:15:51.665569       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1215 08:15:51.665843       1 apf_controller.go:322] Running API Priority and Fairness config worker
I1215 08:15:52.545412       1 controller.go:132] OpenAPI AggregationController: action for item : Nothing (removed from the queue).
I1215 08:15:52.545445       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I1215 08:15:52.548912       1 storage_scheduling.go:109] all system priority classes are created successfully or already exist.
I1215 08:15:53.124003       1 controller.go:611] quota admission added evaluator for: serviceaccounts
I1215 08:15:53.130037       1 controller.go:611] quota admission added evaluator for: deployments.apps
I1215 08:15:53.152462       1 controller.go:611] quota admission added evaluator for: daemonsets.apps
I1215 08:15:53.164379       1 controller.go:611] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1215 08:15:53.168777       1 controller.go:611] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1215 08:15:53.402855       1 controller.go:611] quota admission added evaluator for: events.events.k8s.io
I1215 08:15:54.377098       1 controller.go:611] quota admission added evaluator for: endpoints
I1215 08:16:04.085351       1 controller.go:611] quota admission added evaluator for: endpointslices.discovery.k8s.io
W1215 08:27:53.106403       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
I1215 08:31:53.290117       1 controller.go:611] quota admission added evaluator for: namespaces
I1215 08:31:53.310707       1 controller.go:611] quota admission added evaluator for: replicasets.apps
I1215 08:31:53.473545       1 alloc.go:329] "allocated clusterIPs" service="kubernetes-dashboard/kubernetes-dashboard" clusterIPs=map[IPv4:10.105.73.248]
I1215 08:31:53.483605       1 alloc.go:329] "allocated clusterIPs" service="kubernetes-dashboard/dashboard-metrics-scraper" clusterIPs=map[IPv4:10.104.219.50]
I1215 09:07:51.037180       1 controller.go:611] quota admission added evaluator for: ingresses.networking.k8s.io
I1215 09:07:51.044834       1 alloc.go:329] "allocated clusterIPs" service="default/eddiehub-linkfree-service" clusterIPs=map[IPv4:10.102.63.77]
W1215 09:16:09.280074       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
{"level":"warn","ts":"2022-12-15T09:48:02.551Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000fcca80/#initially=[https://127.0.0.1:2379]","attempt":0,"error":"rpc error: code = Canceled desc = context canceled"}
E1215 09:48:02.551560       1 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled

* 
* ==> kube-apiserver [84d1ba317f9b] <==
* W1215 08:10:21.340594       1 genericapiserver.go:538] Skipping API apps/v1beta2 because it has no resources.
W1215 08:10:21.340618       1 genericapiserver.go:538] Skipping API apps/v1beta1 because it has no resources.
W1215 08:10:21.341740       1 genericapiserver.go:538] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
I1215 08:10:21.343954       1 plugins.go:158] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I1215 08:10:21.343972       1 plugins.go:161] Loaded 11 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,CertificateSubjectRestriction,ValidatingAdmissionWebhook,ResourceQuota.
W1215 08:10:21.360023       1 genericapiserver.go:538] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I1215 08:10:21.986149       1 dynamic_cafile_content.go:156] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1215 08:10:21.986157       1 dynamic_cafile_content.go:156] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1215 08:10:21.986376       1 dynamic_serving_content.go:131] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I1215 08:10:21.986433       1 secure_serving.go:266] Serving securely on [::]:8443
I1215 08:10:21.986484       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1215 08:10:21.986649       1 customresource_discovery_controller.go:209] Starting DiscoveryController
I1215 08:10:21.986753       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I1215 08:10:21.986746       1 apf_controller.go:317] Starting API Priority and Fairness config controller
I1215 08:10:21.986816       1 establishing_controller.go:76] Starting EstablishingController
I1215 08:10:21.986831       1 crd_finalizer.go:266] Starting CRDFinalizer
I1215 08:10:21.986858       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I1215 08:10:21.986950       1 controller.go:83] Starting OpenAPI AggregationController
I1215 08:10:21.987059       1 shared_informer.go:240] Waiting for caches to sync for crd-autoregister
I1215 08:10:21.987147       1 available_controller.go:491] Starting AvailableConditionController
I1215 08:10:21.987173       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I1215 08:10:21.987465       1 controller.go:85] Starting OpenAPI controller
I1215 08:10:21.987957       1 dynamic_serving_content.go:131] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1215 08:10:21.988044       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I1215 08:10:21.988125       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1215 08:10:21.986878       1 naming_controller.go:291] Starting NamingConditionController
I1215 08:10:21.986819       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1215 08:10:21.986888       1 autoregister_controller.go:141] Starting autoregister controller
I1215 08:10:21.988264       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1215 08:10:21.988679       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I1215 08:10:21.988713       1 shared_informer.go:240] Waiting for caches to sync for cluster_authentication_trust_controller
I1215 08:10:21.988794       1 dynamic_cafile_content.go:156] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1215 08:10:21.989200       1 dynamic_cafile_content.go:156] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1215 08:10:22.014884       1 controller.go:611] quota admission added evaluator for: namespaces
I1215 08:10:22.087008       1 apf_controller.go:322] Running API Priority and Fairness config worker
I1215 08:10:22.087232       1 cache.go:39] Caches are synced for AvailableConditionController controller
I1215 08:10:22.088246       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1215 08:10:22.088332       1 cache.go:39] Caches are synced for autoregister controller
I1215 08:10:22.088696       1 shared_informer.go:247] Caches are synced for crd-autoregister 
I1215 08:10:22.088752       1 shared_informer.go:247] Caches are synced for cluster_authentication_trust_controller 
I1215 08:10:22.090008       1 shared_informer.go:247] Caches are synced for node_authorizer 
I1215 08:10:22.986889       1 controller.go:132] OpenAPI AggregationController: action for item : Nothing (removed from the queue).
I1215 08:10:22.986918       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I1215 08:10:22.990969       1 storage_scheduling.go:93] created PriorityClass system-node-critical with value 2000001000
I1215 08:10:22.993390       1 storage_scheduling.go:93] created PriorityClass system-cluster-critical with value 2000000000
I1215 08:10:22.993412       1 storage_scheduling.go:109] all system priority classes are created successfully or already exist.
I1215 08:10:23.272524       1 controller.go:611] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1215 08:10:23.302385       1 controller.go:611] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1215 08:10:23.424984       1 alloc.go:329] "allocated clusterIPs" service="default/kubernetes" clusterIPs=map[IPv4:10.96.0.1]
W1215 08:10:23.428554       1 lease.go:233] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I1215 08:10:23.429217       1 controller.go:611] quota admission added evaluator for: endpoints
I1215 08:10:23.431804       1 controller.go:611] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1215 08:10:24.105949       1 controller.go:611] quota admission added evaluator for: serviceaccounts
I1215 08:10:24.844877       1 controller.go:611] quota admission added evaluator for: deployments.apps
I1215 08:10:24.849833       1 alloc.go:329] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs=map[IPv4:10.96.0.10]
I1215 08:10:24.856930       1 controller.go:611] quota admission added evaluator for: daemonsets.apps
I1215 08:10:24.974168       1 controller.go:611] quota admission added evaluator for: leases.coordination.k8s.io
I1215 08:10:37.012238       1 controller.go:611] quota admission added evaluator for: replicasets.apps
I1215 08:10:37.662043       1 controller.go:611] quota admission added evaluator for: controllerrevisions.apps
I1215 08:10:38.413778       1 controller.go:611] quota admission added evaluator for: events.events.k8s.io

* 
* ==> kube-controller-manager [43b305667b9c] <==
* I1215 08:37:58.434109       1 event.go:294] "Event occurred" object="default/foip-deployment-668448854c" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-668448854c-7xnd9"
I1215 08:38:30.610237       1 event.go:294] "Event occurred" object="default/foip-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set foip-deployment-649bf596d4 to 2"
I1215 08:38:30.614747       1 event.go:294] "Event occurred" object="default/foip-deployment-649bf596d4" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-649bf596d4-6ttkf"
I1215 08:38:30.616607       1 event.go:294] "Event occurred" object="default/foip-deployment-649bf596d4" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-649bf596d4-ln5f4"
I1215 08:39:40.458149       1 event.go:294] "Event occurred" object="default/foip-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set foip-deployment-786b8c6cfb to 2"
I1215 08:39:40.461451       1 event.go:294] "Event occurred" object="default/foip-deployment-786b8c6cfb" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-786b8c6cfb-mx6sf"
I1215 08:39:40.463561       1 event.go:294] "Event occurred" object="default/foip-deployment-786b8c6cfb" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-786b8c6cfb-prddx"
I1215 08:45:51.685249       1 event.go:294] "Event occurred" object="default/foip-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set foip-deployment-786b8c6cfb to 2"
I1215 08:45:51.689074       1 event.go:294] "Event occurred" object="default/foip-deployment-786b8c6cfb" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-786b8c6cfb-qnwkn"
I1215 08:45:51.692709       1 event.go:294] "Event occurred" object="default/foip-deployment-786b8c6cfb" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-786b8c6cfb-9m9rb"
I1215 08:55:17.804536       1 event.go:294] "Event occurred" object="default/foip-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set foip-deployment-786b8c6cfb to 2"
I1215 08:55:17.808261       1 event.go:294] "Event occurred" object="default/foip-deployment-786b8c6cfb" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-786b8c6cfb-wvjm5"
I1215 08:55:17.811961       1 event.go:294] "Event occurred" object="default/foip-deployment-786b8c6cfb" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-786b8c6cfb-kqpf5"
I1215 08:56:38.404158       1 event.go:294] "Event occurred" object="default/foip-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set foip-deployment-5b567b8db to 2"
I1215 08:56:38.408775       1 event.go:294] "Event occurred" object="default/foip-deployment-5b567b8db" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-5b567b8db-59tx5"
I1215 08:56:38.411373       1 event.go:294] "Event occurred" object="default/foip-deployment-5b567b8db" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-5b567b8db-l9hp7"
I1215 09:01:04.341115       1 event.go:294] "Event occurred" object="default/foip-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set foip-deployment-78fbc69869 to 2"
I1215 09:01:04.344621       1 event.go:294] "Event occurred" object="default/foip-deployment-78fbc69869" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-78fbc69869-lw9lk"
I1215 09:01:04.348108       1 event.go:294] "Event occurred" object="default/foip-deployment-78fbc69869" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-78fbc69869-vsjkr"
I1215 09:02:08.710579       1 event.go:294] "Event occurred" object="default/foip-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set foip-deployment-766894f5f8 to 1"
I1215 09:02:08.714731       1 event.go:294] "Event occurred" object="default/foip-deployment-766894f5f8" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-766894f5f8-24tzm"
I1215 09:03:18.991816       1 event.go:294] "Event occurred" object="default/foip-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set foip-deployment-94c9b8775 to 2"
I1215 09:03:18.996469       1 event.go:294] "Event occurred" object="default/foip-deployment-94c9b8775" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-94c9b8775-hzsg5"
I1215 09:03:18.999611       1 event.go:294] "Event occurred" object="default/foip-deployment-94c9b8775" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-94c9b8775-qhgbd"
I1215 09:04:58.995824       1 event.go:294] "Event occurred" object="default/foip-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set foip-deployment-545d6ff4f6 to 2"
I1215 09:04:59.001317       1 event.go:294] "Event occurred" object="default/foip-deployment-545d6ff4f6" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-545d6ff4f6-6xjwg"
I1215 09:04:59.003007       1 event.go:294] "Event occurred" object="default/foip-deployment-545d6ff4f6" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-545d6ff4f6-f4brw"
I1215 09:07:51.042438       1 event.go:294] "Event occurred" object="default/eddiehub-linkfree-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set eddiehub-linkfree-deployment-84d7b6f477 to 2"
I1215 09:07:51.047668       1 event.go:294] "Event occurred" object="default/eddiehub-linkfree-deployment-84d7b6f477" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: eddiehub-linkfree-deployment-84d7b6f477-vfhnk"
I1215 09:07:51.051260       1 event.go:294] "Event occurred" object="default/eddiehub-linkfree-deployment-84d7b6f477" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: eddiehub-linkfree-deployment-84d7b6f477-pcjjs"
I1215 09:15:53.791863       1 cleaner.go:172] Cleaning CSR "csr-jqkxv" as it is more than 1h0m0s old and approved.
I1215 09:28:17.353517       1 event.go:294] "Event occurred" object="default/foip-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set foip-deployment-545d6ff4f6 to 2"
I1215 09:28:17.357501       1 event.go:294] "Event occurred" object="default/foip-deployment-545d6ff4f6" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-545d6ff4f6-8znt7"
I1215 09:28:17.360793       1 event.go:294] "Event occurred" object="default/foip-deployment-545d6ff4f6" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-545d6ff4f6-cqh9l"
I1215 09:30:55.515617       1 event.go:294] "Event occurred" object="default/foip-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set foip-deployment-74dcb9f985 to 2"
I1215 09:30:55.518842       1 event.go:294] "Event occurred" object="default/foip-deployment-74dcb9f985" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-74dcb9f985-s2p6s"
I1215 09:30:55.520768       1 event.go:294] "Event occurred" object="default/foip-deployment-74dcb9f985" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-74dcb9f985-jx97m"
I1215 09:32:44.708829       1 event.go:294] "Event occurred" object="default/foip-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set foip-deployment-74dcb9f985 to 2"
I1215 09:32:44.712947       1 event.go:294] "Event occurred" object="default/foip-deployment-74dcb9f985" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-74dcb9f985-g2qdl"
I1215 09:32:44.718217       1 event.go:294] "Event occurred" object="default/foip-deployment-74dcb9f985" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-74dcb9f985-nhxtm"
I1215 09:33:43.723990       1 event.go:294] "Event occurred" object="default/foip-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set foip-deployment-65b6bd4d5b to 2"
I1215 09:33:43.728293       1 event.go:294] "Event occurred" object="default/foip-deployment-65b6bd4d5b" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-65b6bd4d5b-8d68l"
I1215 09:33:43.730485       1 event.go:294] "Event occurred" object="default/foip-deployment-65b6bd4d5b" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-65b6bd4d5b-ksjwt"
I1215 09:34:22.059474       1 event.go:294] "Event occurred" object="default/foip-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set foip-deployment-74dcb9f985 to 2"
I1215 09:34:22.063989       1 event.go:294] "Event occurred" object="default/foip-deployment-74dcb9f985" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-74dcb9f985-dz9fv"
I1215 09:34:22.065927       1 event.go:294] "Event occurred" object="default/foip-deployment-74dcb9f985" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-74dcb9f985-pjksz"
I1215 09:35:11.917548       1 event.go:294] "Event occurred" object="default/foip-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set foip-deployment-74dcb9f985 to 1"
I1215 09:35:11.921940       1 event.go:294] "Event occurred" object="default/foip-deployment-74dcb9f985" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-74dcb9f985-7qvfc"
I1215 09:35:33.379485       1 event.go:294] "Event occurred" object="default/foip-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set foip-deployment-5f4cdd4454 to 1"
I1215 09:35:33.381963       1 event.go:294] "Event occurred" object="default/foip-deployment-5f4cdd4454" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-5f4cdd4454-cfhtx"
I1215 09:38:15.376308       1 event.go:294] "Event occurred" object="default/foip-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set foip-deployment-685fd58db6 to 1"
I1215 09:38:15.380299       1 event.go:294] "Event occurred" object="default/foip-deployment-685fd58db6" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-685fd58db6-vmsfv"
I1215 09:54:03.024143       1 event.go:294] "Event occurred" object="default/foip-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set foip-deployment-685fd58db6 to 1"
I1215 09:54:03.032886       1 event.go:294] "Event occurred" object="default/foip-deployment-685fd58db6" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-685fd58db6-xczdv"
I1215 10:04:06.920880       1 event.go:294] "Event occurred" object="default/foip-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set foip-deployment-685fd58db6 to 1"
I1215 10:04:06.925285       1 event.go:294] "Event occurred" object="default/foip-deployment-685fd58db6" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-685fd58db6-whswl"
I1215 10:07:30.903416       1 event.go:294] "Event occurred" object="default/foip-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set foip-deployment-56b559654d to 1"
I1215 10:07:30.907605       1 event.go:294] "Event occurred" object="default/foip-deployment-56b559654d" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-56b559654d-rt8vc"
I1215 10:08:22.205157       1 event.go:294] "Event occurred" object="default/foip-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set foip-deployment-685fd58db6 to 1"
I1215 10:08:22.210420       1 event.go:294] "Event occurred" object="default/foip-deployment-685fd58db6" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: foip-deployment-685fd58db6-6d8l2"

* 
* ==> kube-controller-manager [e593bca41b92] <==
* I1215 08:10:36.606984       1 shared_informer.go:240] Waiting for caches to sync for garbage collector
I1215 08:10:36.606996       1 graph_builder.go:289] GraphBuilder running
I1215 08:10:36.607022       1 controllermanager.go:605] Started "garbagecollector"
I1215 08:10:36.857307       1 controllermanager.go:605] Started "root-ca-cert-publisher"
I1215 08:10:36.857409       1 publisher.go:107] Starting root CA certificate configmap publisher
I1215 08:10:36.857425       1 shared_informer.go:240] Waiting for caches to sync for crt configmap
I1215 08:10:36.858907       1 shared_informer.go:240] Waiting for caches to sync for resource quota
W1215 08:10:36.862316       1 actual_state_of_world.go:539] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName="minikube" does not exist
I1215 08:10:36.867684       1 shared_informer.go:247] Caches are synced for node 
I1215 08:10:36.867718       1 range_allocator.go:173] Starting range CIDR allocator
I1215 08:10:36.867722       1 shared_informer.go:240] Waiting for caches to sync for cidrallocator
I1215 08:10:36.867726       1 shared_informer.go:247] Caches are synced for cidrallocator 
I1215 08:10:36.871700       1 shared_informer.go:247] Caches are synced for deployment 
I1215 08:10:36.872537       1 shared_informer.go:240] Waiting for caches to sync for garbage collector
I1215 08:10:36.873759       1 range_allocator.go:374] Set node minikube PodCIDR to [10.244.0.0/24]
I1215 08:10:36.874467       1 shared_informer.go:247] Caches are synced for ReplicationController 
I1215 08:10:36.880282       1 shared_informer.go:247] Caches are synced for disruption 
I1215 08:10:36.880307       1 disruption.go:371] Sending events to api server.
I1215 08:10:36.889990       1 shared_informer.go:247] Caches are synced for ReplicaSet 
I1215 08:10:36.896806       1 shared_informer.go:247] Caches are synced for ClusterRoleAggregator 
I1215 08:10:36.905239       1 shared_informer.go:247] Caches are synced for PVC protection 
I1215 08:10:36.907293       1 shared_informer.go:247] Caches are synced for certificate-csrapproving 
I1215 08:10:36.907595       1 shared_informer.go:247] Caches are synced for TTL after finished 
I1215 08:10:36.908449       1 shared_informer.go:247] Caches are synced for stateful set 
I1215 08:10:36.911549       1 shared_informer.go:247] Caches are synced for PV protection 
I1215 08:10:36.912658       1 shared_informer.go:247] Caches are synced for TTL 
I1215 08:10:36.913412       1 shared_informer.go:247] Caches are synced for namespace 
I1215 08:10:36.918951       1 shared_informer.go:247] Caches are synced for ephemeral 
I1215 08:10:36.924294       1 shared_informer.go:247] Caches are synced for GC 
I1215 08:10:36.928535       1 shared_informer.go:247] Caches are synced for daemon sets 
I1215 08:10:36.931734       1 shared_informer.go:247] Caches are synced for job 
I1215 08:10:36.932865       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-kube-apiserver-client 
I1215 08:10:36.932891       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-kubelet-client 
I1215 08:10:36.932916       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-kubelet-serving 
I1215 08:10:36.932959       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-legacy-unknown 
I1215 08:10:36.937234       1 shared_informer.go:247] Caches are synced for taint 
I1215 08:10:36.937306       1 node_lifecycle_controller.go:1397] Initializing eviction metric for zone: 
I1215 08:10:36.937305       1 taint_manager.go:187] "Starting NoExecuteTaintManager"
W1215 08:10:36.937343       1 node_lifecycle_controller.go:1012] Missing timestamp for Node minikube. Assuming now as a timestamp.
I1215 08:10:36.937379       1 node_lifecycle_controller.go:1213] Controller detected that zone  is now in state Normal.
I1215 08:10:36.937445       1 event.go:294] "Event occurred" object="minikube" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I1215 08:10:36.940957       1 shared_informer.go:247] Caches are synced for persistent volume 
I1215 08:10:36.944438       1 shared_informer.go:247] Caches are synced for expand 
I1215 08:10:36.954772       1 shared_informer.go:247] Caches are synced for bootstrap_signer 
I1215 08:10:36.955827       1 shared_informer.go:247] Caches are synced for service account 
I1215 08:10:36.957501       1 shared_informer.go:247] Caches are synced for crt configmap 
I1215 08:10:36.957529       1 shared_informer.go:247] Caches are synced for endpoint_slice 
I1215 08:10:36.958683       1 shared_informer.go:247] Caches are synced for attach detach 
I1215 08:10:36.968278       1 shared_informer.go:247] Caches are synced for HPA 
I1215 08:10:37.014891       1 event.go:294] "Event occurred" object="kube-system/coredns" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set coredns-64897985d to 1"
I1215 08:10:37.119577       1 shared_informer.go:247] Caches are synced for endpoint 
I1215 08:10:37.154471       1 shared_informer.go:247] Caches are synced for endpoint_slice_mirroring 
I1215 08:10:37.159359       1 shared_informer.go:247] Caches are synced for resource quota 
I1215 08:10:37.164017       1 shared_informer.go:247] Caches are synced for resource quota 
I1215 08:10:37.206382       1 shared_informer.go:247] Caches are synced for cronjob 
I1215 08:10:37.573677       1 shared_informer.go:247] Caches are synced for garbage collector 
I1215 08:10:37.607262       1 shared_informer.go:247] Caches are synced for garbage collector 
I1215 08:10:37.607295       1 garbagecollector.go:155] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
I1215 08:10:37.668451       1 event.go:294] "Event occurred" object="kube-system/kube-proxy" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kube-proxy-fgd5f"
I1215 08:10:37.912884       1 event.go:294] "Event occurred" object="kube-system/coredns-64897985d" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-64897985d-2t96b"

* 
* ==> kube-proxy [5be6d078984f] <==
* E1215 08:10:38.383466       1 proxier.go:647] "Failed to read builtin modules file, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" err="open /lib/modules/5.10.16.3-microsoft-standard-WSL2/modules.builtin: no such file or directory" filePath="/lib/modules/5.10.16.3-microsoft-standard-WSL2/modules.builtin"
I1215 08:10:38.385223       1 proxier.go:657] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs"
I1215 08:10:38.386367       1 proxier.go:657] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_rr"
I1215 08:10:38.387270       1 proxier.go:657] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_wrr"
I1215 08:10:38.388085       1 proxier.go:657] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_sh"
I1215 08:10:38.389045       1 proxier.go:657] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="nf_conntrack"
I1215 08:10:38.395668       1 node.go:163] Successfully retrieved node IP: 192.168.49.2
I1215 08:10:38.395702       1 server_others.go:138] "Detected node IP" address="192.168.49.2"
I1215 08:10:38.395726       1 server_others.go:561] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I1215 08:10:38.411105       1 server_others.go:206] "Using iptables Proxier"
I1215 08:10:38.411136       1 server_others.go:213] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I1215 08:10:38.411141       1 server_others.go:214] "Creating dualStackProxier for iptables"
I1215 08:10:38.411157       1 server_others.go:491] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I1215 08:10:38.411429       1 server.go:656] "Version info" version="v1.23.6"
I1215 08:10:38.412033       1 config.go:317] "Starting service config controller"
I1215 08:10:38.412052       1 config.go:226] "Starting endpoint slice config controller"
I1215 08:10:38.412064       1 shared_informer.go:240] Waiting for caches to sync for service config
I1215 08:10:38.412064       1 shared_informer.go:240] Waiting for caches to sync for endpoint slice config
I1215 08:10:38.512383       1 shared_informer.go:247] Caches are synced for endpoint slice config 
I1215 08:10:38.512395       1 shared_informer.go:247] Caches are synced for service config 

* 
* ==> kube-proxy [66f8efe90cc9] <==
* E1215 08:15:53.373577       1 proxier.go:647] "Failed to read builtin modules file, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" err="open /lib/modules/5.10.16.3-microsoft-standard-WSL2/modules.builtin: no such file or directory" filePath="/lib/modules/5.10.16.3-microsoft-standard-WSL2/modules.builtin"
I1215 08:15:53.375388       1 proxier.go:657] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs"
I1215 08:15:53.376453       1 proxier.go:657] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_rr"
I1215 08:15:53.377318       1 proxier.go:657] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_wrr"
I1215 08:15:53.378302       1 proxier.go:657] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_sh"
I1215 08:15:53.379087       1 proxier.go:657] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="nf_conntrack"
I1215 08:15:53.386465       1 node.go:163] Successfully retrieved node IP: 192.168.49.2
I1215 08:15:53.386498       1 server_others.go:138] "Detected node IP" address="192.168.49.2"
I1215 08:15:53.386524       1 server_others.go:561] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I1215 08:15:53.400839       1 server_others.go:206] "Using iptables Proxier"
I1215 08:15:53.400878       1 server_others.go:213] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I1215 08:15:53.400882       1 server_others.go:214] "Creating dualStackProxier for iptables"
I1215 08:15:53.400894       1 server_others.go:491] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I1215 08:15:53.401147       1 server.go:656] "Version info" version="v1.23.6"
I1215 08:15:53.401475       1 config.go:317] "Starting service config controller"
I1215 08:15:53.401503       1 shared_informer.go:240] Waiting for caches to sync for service config
I1215 08:15:53.401592       1 config.go:226] "Starting endpoint slice config controller"
I1215 08:15:53.401632       1 shared_informer.go:240] Waiting for caches to sync for endpoint slice config
I1215 08:15:53.501619       1 shared_informer.go:247] Caches are synced for service config 
I1215 08:15:53.502774       1 shared_informer.go:247] Caches are synced for endpoint slice config 

* 
* ==> kube-scheduler [07c0346d9600] <==
* I1215 08:10:20.309628       1 serving.go:348] Generated self-signed cert in-memory
W1215 08:10:21.996812       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1215 08:10:21.996885       1 authentication.go:345] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1215 08:10:21.996895       1 authentication.go:346] Continuing without authentication configuration. This may treat all requests as anonymous.
W1215 08:10:21.996903       1 authentication.go:347] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1215 08:10:22.091740       1 server.go:139] "Starting Kubernetes Scheduler" version="v1.23.6"
I1215 08:10:22.093121       1 configmap_cafile_content.go:201] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1215 08:10:22.093150       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1215 08:10:22.093158       1 secure_serving.go:200] Serving securely on 127.0.0.1:10259
I1215 08:10:22.093211       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
W1215 08:10:22.166006       1 reflector.go:324] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:205: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W1215 08:10:22.166097       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1215 08:10:22.166108       1 reflector.go:138] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:205: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1215 08:10:22.166120       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1215 08:10:22.166217       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1215 08:10:22.166250       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W1215 08:10:22.166236       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1215 08:10:22.166341       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W1215 08:10:22.166308       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1215 08:10:22.166443       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1215 08:10:22.166469       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1215 08:10:22.166480       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W1215 08:10:22.166514       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1215 08:10:22.166548       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W1215 08:10:22.168057       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1215 08:10:22.168106       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1215 08:10:22.168117       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1215 08:10:22.168153       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W1215 08:10:22.168219       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1215 08:10:22.168316       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W1215 08:10:22.168366       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1215 08:10:22.168377       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1215 08:10:22.168406       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1215 08:10:22.168413       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W1215 08:10:22.169337       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1215 08:10:22.169375       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1beta1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1215 08:10:22.169378       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1215 08:10:22.169388       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1beta1.CSIStorageCapacity: failed to list *v1beta1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1215 08:10:22.169410       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1215 08:10:22.169447       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W1215 08:10:22.992238       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1215 08:10:22.992267       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1215 08:10:23.022290       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1215 08:10:23.022317       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W1215 08:10:23.036152       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1215 08:10:23.036180       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W1215 08:10:23.047380       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1215 08:10:23.047408       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1215 08:10:23.152627       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1215 08:10:23.152677       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
I1215 08:10:23.593899       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file 

* 
* ==> kube-scheduler [304e517aff5f] <==
* I1215 08:15:50.083692       1 serving.go:348] Generated self-signed cert in-memory
W1215 08:15:51.550559       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1215 08:15:51.550594       1 authentication.go:345] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1215 08:15:51.550602       1 authentication.go:346] Continuing without authentication configuration. This may treat all requests as anonymous.
W1215 08:15:51.550607       1 authentication.go:347] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1215 08:15:51.667789       1 server.go:139] "Starting Kubernetes Scheduler" version="v1.23.6"
I1215 08:15:51.669112       1 configmap_cafile_content.go:201] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1215 08:15:51.669148       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1215 08:15:51.669156       1 secure_serving.go:200] Serving securely on 127.0.0.1:10259
I1215 08:15:51.669277       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1215 08:15:51.770163       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file 

* 
* ==> kubelet <==
* -- Logs begin at Thu 2022-12-15 08:15:36 UTC, end at Thu 2022-12-15 10:13:44 UTC. --
Dec 15 10:08:39 minikube kubelet[1158]: I1215 10:08:39.151704    1158 scope.go:110] "RemoveContainer" containerID="9d5d7b071fbd7b1cc37eb5a9dc99d5a0b120cfd4825a39b95cba65f2102712be"
Dec 15 10:08:39 minikube kubelet[1158]: E1215 10:08:39.151848    1158 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with CrashLoopBackOff: \"back-off 20s restarting failed container=api pod=foip-deployment-685fd58db6-6d8l2_default(abe825b5-da94-4ccd-bb48-b4d6a737b63e)\"" pod="default/foip-deployment-685fd58db6-6d8l2" podUID=abe825b5-da94-4ccd-bb48-b4d6a737b63e
Dec 15 10:08:40 minikube kubelet[1158]: I1215 10:08:40.158080    1158 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/foip-deployment-685fd58db6-6d8l2 through plugin: invalid network status for"
Dec 15 10:08:52 minikube kubelet[1158]: I1215 10:08:52.314479    1158 scope.go:110] "RemoveContainer" containerID="9d5d7b071fbd7b1cc37eb5a9dc99d5a0b120cfd4825a39b95cba65f2102712be"
Dec 15 10:08:52 minikube kubelet[1158]: E1215 10:08:52.314653    1158 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with CrashLoopBackOff: \"back-off 20s restarting failed container=api pod=foip-deployment-685fd58db6-6d8l2_default(abe825b5-da94-4ccd-bb48-b4d6a737b63e)\"" pod="default/foip-deployment-685fd58db6-6d8l2" podUID=abe825b5-da94-4ccd-bb48-b4d6a737b63e
Dec 15 10:09:04 minikube kubelet[1158]: I1215 10:09:04.314503    1158 scope.go:110] "RemoveContainer" containerID="9d5d7b071fbd7b1cc37eb5a9dc99d5a0b120cfd4825a39b95cba65f2102712be"
Dec 15 10:09:05 minikube kubelet[1158]: I1215 10:09:05.291574    1158 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/foip-deployment-685fd58db6-6d8l2 through plugin: invalid network status for"
Dec 15 10:09:05 minikube kubelet[1158]: I1215 10:09:05.294578    1158 scope.go:110] "RemoveContainer" containerID="9d5d7b071fbd7b1cc37eb5a9dc99d5a0b120cfd4825a39b95cba65f2102712be"
Dec 15 10:09:05 minikube kubelet[1158]: I1215 10:09:05.294843    1158 scope.go:110] "RemoveContainer" containerID="969f3caf238c5423e234c45d8a72d4f74d03831b6109f190ae74986112961e53"
Dec 15 10:09:05 minikube kubelet[1158]: E1215 10:09:05.294988    1158 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with CrashLoopBackOff: \"back-off 40s restarting failed container=api pod=foip-deployment-685fd58db6-6d8l2_default(abe825b5-da94-4ccd-bb48-b4d6a737b63e)\"" pod="default/foip-deployment-685fd58db6-6d8l2" podUID=abe825b5-da94-4ccd-bb48-b4d6a737b63e
Dec 15 10:09:06 minikube kubelet[1158]: I1215 10:09:06.301049    1158 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/foip-deployment-685fd58db6-6d8l2 through plugin: invalid network status for"
Dec 15 10:09:20 minikube kubelet[1158]: I1215 10:09:20.314278    1158 scope.go:110] "RemoveContainer" containerID="969f3caf238c5423e234c45d8a72d4f74d03831b6109f190ae74986112961e53"
Dec 15 10:09:20 minikube kubelet[1158]: E1215 10:09:20.314448    1158 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with CrashLoopBackOff: \"back-off 40s restarting failed container=api pod=foip-deployment-685fd58db6-6d8l2_default(abe825b5-da94-4ccd-bb48-b4d6a737b63e)\"" pod="default/foip-deployment-685fd58db6-6d8l2" podUID=abe825b5-da94-4ccd-bb48-b4d6a737b63e
Dec 15 10:09:35 minikube kubelet[1158]: I1215 10:09:35.314187    1158 scope.go:110] "RemoveContainer" containerID="969f3caf238c5423e234c45d8a72d4f74d03831b6109f190ae74986112961e53"
Dec 15 10:09:35 minikube kubelet[1158]: E1215 10:09:35.314367    1158 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with CrashLoopBackOff: \"back-off 40s restarting failed container=api pod=foip-deployment-685fd58db6-6d8l2_default(abe825b5-da94-4ccd-bb48-b4d6a737b63e)\"" pod="default/foip-deployment-685fd58db6-6d8l2" podUID=abe825b5-da94-4ccd-bb48-b4d6a737b63e
Dec 15 10:09:48 minikube kubelet[1158]: I1215 10:09:48.314802    1158 scope.go:110] "RemoveContainer" containerID="969f3caf238c5423e234c45d8a72d4f74d03831b6109f190ae74986112961e53"
Dec 15 10:09:48 minikube kubelet[1158]: I1215 10:09:48.506832    1158 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/foip-deployment-685fd58db6-6d8l2 through plugin: invalid network status for"
Dec 15 10:09:48 minikube kubelet[1158]: I1215 10:09:48.510249    1158 scope.go:110] "RemoveContainer" containerID="969f3caf238c5423e234c45d8a72d4f74d03831b6109f190ae74986112961e53"
Dec 15 10:09:48 minikube kubelet[1158]: I1215 10:09:48.510449    1158 scope.go:110] "RemoveContainer" containerID="109dddfc47efd55a5e4332e314cd9e2dc25d2e4bc83df33851a25fc4a55292b2"
Dec 15 10:09:48 minikube kubelet[1158]: E1215 10:09:48.510607    1158 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=api pod=foip-deployment-685fd58db6-6d8l2_default(abe825b5-da94-4ccd-bb48-b4d6a737b63e)\"" pod="default/foip-deployment-685fd58db6-6d8l2" podUID=abe825b5-da94-4ccd-bb48-b4d6a737b63e
Dec 15 10:09:49 minikube kubelet[1158]: I1215 10:09:49.516439    1158 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/foip-deployment-685fd58db6-6d8l2 through plugin: invalid network status for"
Dec 15 10:10:00 minikube kubelet[1158]: I1215 10:10:00.314374    1158 scope.go:110] "RemoveContainer" containerID="109dddfc47efd55a5e4332e314cd9e2dc25d2e4bc83df33851a25fc4a55292b2"
Dec 15 10:10:00 minikube kubelet[1158]: E1215 10:10:00.314557    1158 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=api pod=foip-deployment-685fd58db6-6d8l2_default(abe825b5-da94-4ccd-bb48-b4d6a737b63e)\"" pod="default/foip-deployment-685fd58db6-6d8l2" podUID=abe825b5-da94-4ccd-bb48-b4d6a737b63e
Dec 15 10:10:13 minikube kubelet[1158]: I1215 10:10:13.314829    1158 scope.go:110] "RemoveContainer" containerID="109dddfc47efd55a5e4332e314cd9e2dc25d2e4bc83df33851a25fc4a55292b2"
Dec 15 10:10:13 minikube kubelet[1158]: E1215 10:10:13.314987    1158 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=api pod=foip-deployment-685fd58db6-6d8l2_default(abe825b5-da94-4ccd-bb48-b4d6a737b63e)\"" pod="default/foip-deployment-685fd58db6-6d8l2" podUID=abe825b5-da94-4ccd-bb48-b4d6a737b63e
Dec 15 10:10:27 minikube kubelet[1158]: I1215 10:10:27.314565    1158 scope.go:110] "RemoveContainer" containerID="109dddfc47efd55a5e4332e314cd9e2dc25d2e4bc83df33851a25fc4a55292b2"
Dec 15 10:10:27 minikube kubelet[1158]: E1215 10:10:27.314773    1158 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=api pod=foip-deployment-685fd58db6-6d8l2_default(abe825b5-da94-4ccd-bb48-b4d6a737b63e)\"" pod="default/foip-deployment-685fd58db6-6d8l2" podUID=abe825b5-da94-4ccd-bb48-b4d6a737b63e
Dec 15 10:10:42 minikube kubelet[1158]: I1215 10:10:42.314830    1158 scope.go:110] "RemoveContainer" containerID="109dddfc47efd55a5e4332e314cd9e2dc25d2e4bc83df33851a25fc4a55292b2"
Dec 15 10:10:42 minikube kubelet[1158]: E1215 10:10:42.315021    1158 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=api pod=foip-deployment-685fd58db6-6d8l2_default(abe825b5-da94-4ccd-bb48-b4d6a737b63e)\"" pod="default/foip-deployment-685fd58db6-6d8l2" podUID=abe825b5-da94-4ccd-bb48-b4d6a737b63e
Dec 15 10:10:46 minikube kubelet[1158]: W1215 10:10:46.497343    1158 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Dec 15 10:10:57 minikube kubelet[1158]: I1215 10:10:57.314368    1158 scope.go:110] "RemoveContainer" containerID="109dddfc47efd55a5e4332e314cd9e2dc25d2e4bc83df33851a25fc4a55292b2"
Dec 15 10:10:57 minikube kubelet[1158]: E1215 10:10:57.314557    1158 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=api pod=foip-deployment-685fd58db6-6d8l2_default(abe825b5-da94-4ccd-bb48-b4d6a737b63e)\"" pod="default/foip-deployment-685fd58db6-6d8l2" podUID=abe825b5-da94-4ccd-bb48-b4d6a737b63e
Dec 15 10:11:12 minikube kubelet[1158]: I1215 10:11:12.313972    1158 scope.go:110] "RemoveContainer" containerID="109dddfc47efd55a5e4332e314cd9e2dc25d2e4bc83df33851a25fc4a55292b2"
Dec 15 10:11:12 minikube kubelet[1158]: I1215 10:11:12.953222    1158 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/foip-deployment-685fd58db6-6d8l2 through plugin: invalid network status for"
Dec 15 10:11:12 minikube kubelet[1158]: I1215 10:11:12.956605    1158 scope.go:110] "RemoveContainer" containerID="109dddfc47efd55a5e4332e314cd9e2dc25d2e4bc83df33851a25fc4a55292b2"
Dec 15 10:11:12 minikube kubelet[1158]: I1215 10:11:12.956867    1158 scope.go:110] "RemoveContainer" containerID="f446c077de8116bc04bd83818e5b3821d893bd669e2c75806bf3e98afa5a493b"
Dec 15 10:11:12 minikube kubelet[1158]: E1215 10:11:12.957019    1158 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=api pod=foip-deployment-685fd58db6-6d8l2_default(abe825b5-da94-4ccd-bb48-b4d6a737b63e)\"" pod="default/foip-deployment-685fd58db6-6d8l2" podUID=abe825b5-da94-4ccd-bb48-b4d6a737b63e
Dec 15 10:11:13 minikube kubelet[1158]: I1215 10:11:13.963483    1158 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/foip-deployment-685fd58db6-6d8l2 through plugin: invalid network status for"
Dec 15 10:11:27 minikube kubelet[1158]: I1215 10:11:27.314444    1158 scope.go:110] "RemoveContainer" containerID="f446c077de8116bc04bd83818e5b3821d893bd669e2c75806bf3e98afa5a493b"
Dec 15 10:11:27 minikube kubelet[1158]: E1215 10:11:27.314629    1158 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=api pod=foip-deployment-685fd58db6-6d8l2_default(abe825b5-da94-4ccd-bb48-b4d6a737b63e)\"" pod="default/foip-deployment-685fd58db6-6d8l2" podUID=abe825b5-da94-4ccd-bb48-b4d6a737b63e
Dec 15 10:11:38 minikube kubelet[1158]: I1215 10:11:38.314604    1158 scope.go:110] "RemoveContainer" containerID="f446c077de8116bc04bd83818e5b3821d893bd669e2c75806bf3e98afa5a493b"
Dec 15 10:11:38 minikube kubelet[1158]: E1215 10:11:38.314756    1158 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=api pod=foip-deployment-685fd58db6-6d8l2_default(abe825b5-da94-4ccd-bb48-b4d6a737b63e)\"" pod="default/foip-deployment-685fd58db6-6d8l2" podUID=abe825b5-da94-4ccd-bb48-b4d6a737b63e
Dec 15 10:11:50 minikube kubelet[1158]: I1215 10:11:50.314298    1158 scope.go:110] "RemoveContainer" containerID="f446c077de8116bc04bd83818e5b3821d893bd669e2c75806bf3e98afa5a493b"
Dec 15 10:11:50 minikube kubelet[1158]: E1215 10:11:50.314455    1158 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=api pod=foip-deployment-685fd58db6-6d8l2_default(abe825b5-da94-4ccd-bb48-b4d6a737b63e)\"" pod="default/foip-deployment-685fd58db6-6d8l2" podUID=abe825b5-da94-4ccd-bb48-b4d6a737b63e
Dec 15 10:12:03 minikube kubelet[1158]: I1215 10:12:03.314658    1158 scope.go:110] "RemoveContainer" containerID="f446c077de8116bc04bd83818e5b3821d893bd669e2c75806bf3e98afa5a493b"
Dec 15 10:12:03 minikube kubelet[1158]: E1215 10:12:03.314844    1158 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=api pod=foip-deployment-685fd58db6-6d8l2_default(abe825b5-da94-4ccd-bb48-b4d6a737b63e)\"" pod="default/foip-deployment-685fd58db6-6d8l2" podUID=abe825b5-da94-4ccd-bb48-b4d6a737b63e
Dec 15 10:12:14 minikube kubelet[1158]: I1215 10:12:14.314537    1158 scope.go:110] "RemoveContainer" containerID="f446c077de8116bc04bd83818e5b3821d893bd669e2c75806bf3e98afa5a493b"
Dec 15 10:12:14 minikube kubelet[1158]: E1215 10:12:14.314724    1158 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=api pod=foip-deployment-685fd58db6-6d8l2_default(abe825b5-da94-4ccd-bb48-b4d6a737b63e)\"" pod="default/foip-deployment-685fd58db6-6d8l2" podUID=abe825b5-da94-4ccd-bb48-b4d6a737b63e
Dec 15 10:12:26 minikube kubelet[1158]: I1215 10:12:26.314141    1158 scope.go:110] "RemoveContainer" containerID="f446c077de8116bc04bd83818e5b3821d893bd669e2c75806bf3e98afa5a493b"
Dec 15 10:12:26 minikube kubelet[1158]: E1215 10:12:26.314280    1158 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=api pod=foip-deployment-685fd58db6-6d8l2_default(abe825b5-da94-4ccd-bb48-b4d6a737b63e)\"" pod="default/foip-deployment-685fd58db6-6d8l2" podUID=abe825b5-da94-4ccd-bb48-b4d6a737b63e
Dec 15 10:12:39 minikube kubelet[1158]: I1215 10:12:39.314288    1158 scope.go:110] "RemoveContainer" containerID="f446c077de8116bc04bd83818e5b3821d893bd669e2c75806bf3e98afa5a493b"
Dec 15 10:12:39 minikube kubelet[1158]: E1215 10:12:39.314461    1158 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=api pod=foip-deployment-685fd58db6-6d8l2_default(abe825b5-da94-4ccd-bb48-b4d6a737b63e)\"" pod="default/foip-deployment-685fd58db6-6d8l2" podUID=abe825b5-da94-4ccd-bb48-b4d6a737b63e
Dec 15 10:12:53 minikube kubelet[1158]: I1215 10:12:53.314350    1158 scope.go:110] "RemoveContainer" containerID="f446c077de8116bc04bd83818e5b3821d893bd669e2c75806bf3e98afa5a493b"
Dec 15 10:12:53 minikube kubelet[1158]: E1215 10:12:53.314524    1158 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=api pod=foip-deployment-685fd58db6-6d8l2_default(abe825b5-da94-4ccd-bb48-b4d6a737b63e)\"" pod="default/foip-deployment-685fd58db6-6d8l2" podUID=abe825b5-da94-4ccd-bb48-b4d6a737b63e
Dec 15 10:13:08 minikube kubelet[1158]: I1215 10:13:08.314995    1158 scope.go:110] "RemoveContainer" containerID="f446c077de8116bc04bd83818e5b3821d893bd669e2c75806bf3e98afa5a493b"
Dec 15 10:13:08 minikube kubelet[1158]: E1215 10:13:08.315166    1158 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=api pod=foip-deployment-685fd58db6-6d8l2_default(abe825b5-da94-4ccd-bb48-b4d6a737b63e)\"" pod="default/foip-deployment-685fd58db6-6d8l2" podUID=abe825b5-da94-4ccd-bb48-b4d6a737b63e
Dec 15 10:13:22 minikube kubelet[1158]: I1215 10:13:22.314256    1158 scope.go:110] "RemoveContainer" containerID="f446c077de8116bc04bd83818e5b3821d893bd669e2c75806bf3e98afa5a493b"
Dec 15 10:13:22 minikube kubelet[1158]: E1215 10:13:22.314438    1158 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=api pod=foip-deployment-685fd58db6-6d8l2_default(abe825b5-da94-4ccd-bb48-b4d6a737b63e)\"" pod="default/foip-deployment-685fd58db6-6d8l2" podUID=abe825b5-da94-4ccd-bb48-b4d6a737b63e
Dec 15 10:13:34 minikube kubelet[1158]: I1215 10:13:34.314807    1158 scope.go:110] "RemoveContainer" containerID="f446c077de8116bc04bd83818e5b3821d893bd669e2c75806bf3e98afa5a493b"
Dec 15 10:13:34 minikube kubelet[1158]: E1215 10:13:34.315000    1158 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=api pod=foip-deployment-685fd58db6-6d8l2_default(abe825b5-da94-4ccd-bb48-b4d6a737b63e)\"" pod="default/foip-deployment-685fd58db6-6d8l2" podUID=abe825b5-da94-4ccd-bb48-b4d6a737b63e

* 
* ==> kubernetes-dashboard [c6537b1d78ad] <==
* 2022/12/15 10:08:35 [2022-12-15T10:08:35Z] Incoming HTTP/1.1 GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2022/12/15 10:08:35 Getting list of all pet sets in the cluster
2022/12/15 10:08:35 [2022-12-15T10:08:35Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/15 10:08:35 [2022-12-15T10:08:35Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/15 10:08:35 received 0 resources from sidecar instead of 1
2022/12/15 10:08:35 received 0 resources from sidecar instead of 1
2022/12/15 10:08:35 [2022-12-15T10:08:35Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/15 10:08:35 [2022-12-15T10:08:35Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/15 10:08:35 received 0 resources from sidecar instead of 1
2022/12/15 10:08:35 received 0 resources from sidecar instead of 1
2022/12/15 10:08:35 Skipping metric because of error: Metric label not set.
2022/12/15 10:08:35 Skipping metric because of error: Metric label not set.
2022/12/15 10:08:35 [2022-12-15T10:08:35Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/15 10:08:36 [2022-12-15T10:08:36Z] Incoming HTTP/1.1 GET /api/v1/login/status request from 127.0.0.1: 
2022/12/15 10:08:36 [2022-12-15T10:08:36Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/15 10:08:36 [2022-12-15T10:08:36Z] Incoming HTTP/1.1 GET /api/v1/pod/default/foip-deployment-685fd58db6-6d8l2/event?itemsPerPage=10&page=1&sortBy=d,lastSeen request from 127.0.0.1: 
2022/12/15 10:08:36 Getting events related to a pod in namespace
2022/12/15 10:08:36 [2022-12-15T10:08:36Z] Incoming HTTP/1.1 GET /api/v1/pod/default/foip-deployment-685fd58db6-6d8l2 request from 127.0.0.1: 
2022/12/15 10:08:36 Getting details of foip-deployment-685fd58db6-6d8l2 pod in default namespace
2022/12/15 10:08:36 [2022-12-15T10:08:36Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/15 10:08:36 received 0 resources from sidecar instead of 1
2022/12/15 10:08:36 received 0 resources from sidecar instead of 1
2022/12/15 10:08:36 No persistentvolumeclaims found related to foip-deployment-685fd58db6-6d8l2 pod
2022/12/15 10:08:36 [2022-12-15T10:08:36Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/15 10:08:36 [2022-12-15T10:08:36Z] Incoming HTTP/1.1 GET /api/v1/pod/default/foip-deployment-685fd58db6-6d8l2/persistentvolumeclaim?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2022/12/15 10:08:36 No persistentvolumeclaims found related to foip-deployment-685fd58db6-6d8l2 pod
2022/12/15 10:08:36 [2022-12-15T10:08:36Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/15 10:08:37 [2022-12-15T10:08:37Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2022/12/15 10:08:37 Getting list of namespaces
2022/12/15 10:08:37 [2022-12-15T10:08:37Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/15 10:08:41 [2022-12-15T10:08:41Z] Incoming HTTP/1.1 GET /api/v1/pod/default/foip-deployment-685fd58db6-6d8l2 request from 127.0.0.1: 
2022/12/15 10:08:41 Getting details of foip-deployment-685fd58db6-6d8l2 pod in default namespace
2022/12/15 10:08:41 [2022-12-15T10:08:41Z] Incoming HTTP/1.1 GET /api/v1/pod/default/foip-deployment-685fd58db6-6d8l2/event?itemsPerPage=10&page=1&sortBy=d,lastSeen request from 127.0.0.1: 
2022/12/15 10:08:41 Getting events related to a pod in namespace
2022/12/15 10:08:41 [2022-12-15T10:08:41Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/15 10:08:41 received 0 resources from sidecar instead of 1
2022/12/15 10:08:41 received 0 resources from sidecar instead of 1
2022/12/15 10:08:41 No persistentvolumeclaims found related to foip-deployment-685fd58db6-6d8l2 pod
2022/12/15 10:08:41 [2022-12-15T10:08:41Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/15 10:08:41 [2022-12-15T10:08:41Z] Incoming HTTP/1.1 GET /api/v1/pod/default/foip-deployment-685fd58db6-6d8l2/persistentvolumeclaim?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2022/12/15 10:08:41 No persistentvolumeclaims found related to foip-deployment-685fd58db6-6d8l2 pod
2022/12/15 10:08:41 [2022-12-15T10:08:41Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/15 10:08:42 [2022-12-15T10:08:42Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2022/12/15 10:08:42 Getting list of namespaces
2022/12/15 10:08:42 [2022-12-15T10:08:42Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/15 10:08:45 [2022-12-15T10:08:45Z] Incoming HTTP/1.1 GET /api/v1/pod/default/foip-deployment-685fd58db6-6d8l2/persistentvolumeclaim?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2022/12/15 10:08:45 [2022-12-15T10:08:45Z] Incoming HTTP/1.1 GET /api/v1/pod/default/foip-deployment-685fd58db6-6d8l2 request from 127.0.0.1: 
2022/12/15 10:08:45 Getting details of foip-deployment-685fd58db6-6d8l2 pod in default namespace
2022/12/15 10:08:45 [2022-12-15T10:08:45Z] Incoming HTTP/1.1 GET /api/v1/pod/default/foip-deployment-685fd58db6-6d8l2/event?itemsPerPage=10&page=1&sortBy=d,lastSeen request from 127.0.0.1: 
2022/12/15 10:08:45 Getting events related to a pod in namespace
2022/12/15 10:08:45 [2022-12-15T10:08:45Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2022/12/15 10:08:45 Getting list of namespaces
2022/12/15 10:08:45 No persistentvolumeclaims found related to foip-deployment-685fd58db6-6d8l2 pod
2022/12/15 10:08:45 [2022-12-15T10:08:45Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/15 10:08:45 [2022-12-15T10:08:45Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/15 10:08:45 [2022-12-15T10:08:45Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/15 10:08:45 received 0 resources from sidecar instead of 1
2022/12/15 10:08:45 received 0 resources from sidecar instead of 1
2022/12/15 10:08:45 No persistentvolumeclaims found related to foip-deployment-685fd58db6-6d8l2 pod
2022/12/15 10:08:45 [2022-12-15T10:08:45Z] Outcoming response to 127.0.0.1 with 200 status code

* 
* ==> storage-provisioner [a066295e0cb7] <==
* I1215 08:15:53.987808       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1215 08:16:15.018895       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

* 
* ==> storage-provisioner [c0422f17b127] <==
* I1215 08:16:29.465006       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1215 08:16:29.475074       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1215 08:16:29.475117       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1215 08:16:46.891069       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1215 08:16:46.891216       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_a7027167-837b-45ba-b9d5-dce1cddc3bcf!
I1215 08:16:46.891215       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"45b3d65d-82ee-4857-be7c-f63d663cf71d", APIVersion:"v1", ResourceVersion:"711", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_a7027167-837b-45ba-b9d5-dce1cddc3bcf became leader
I1215 08:16:46.991847       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_a7027167-837b-45ba-b9d5-dce1cddc3bcf!

